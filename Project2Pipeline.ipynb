{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d484bd",
   "metadata": {},
   "source": [
    "# Project 2 NLP: Hatespeech Classifier\n",
    "\n",
    "## Authors:\n",
    "\n",
    "Adrian Obermühlner & Freja Rasmussen\n",
    "\n",
    "## Resarch Question:\n",
    "\n",
    "How do different preprocessing methods (nothing, stop word removal, lemming, stemming,…) affect the result of a hate speech classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712926f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "71084f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizing\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b7be0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b6cdf",
   "metadata": {},
   "source": [
    "## Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "198a399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BINARY_LABEL = \"is_hate\"\n",
    "CATEGORIES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "np.random.seed(RANDOM_SEED)  # set random seed for reproducibility\n",
    "# Make the labels into hate and no hate as 1 and 0\n",
    "\n",
    "def binarize_labels(df):\n",
    "    return (df[CATEGORIES].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "data_train = pd.read_csv(\"./data/train/train.csv\", index_col=0)\n",
    "data_train[BINARY_LABEL] = binarize_labels(data_train)\n",
    "\n",
    "data_test = pd.read_csv(\"./data/test/test.csv\", index_col=0).join(\n",
    "    pd.read_csv(\"./data/test_labels/test_labels.csv\", index_col=0)\n",
    ")\n",
    "data_test.drop(data_test[data_test[\"toxic\"] == -1].index, inplace=True)\n",
    "data_test[BINARY_LABEL] = binarize_labels(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eb59a0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0000997932d777bf    Explanation\\nWhy the edits made under my usern...\n",
       "000103f0d9cfb60f    D'aww! He matches this background colour I'm s...\n",
       "000113f07ec002fd    Hey man, I'm really not trying to edit war. It...\n",
       "0001b41b1c6bb37e    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "0001d958c54c6e35    You, sir, are my hero. Any chance you remember...\n",
       "00025465d4725e87    \"\\n\\nCongratulations from me as well, use the ...\n",
       "0002bcb3da6cb337         COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
       "00031b1e95af7921    Your vandalism to the Matt Shirvington article...\n",
       "00037261f536c51d    Sorry if the word 'nonsense' was offensive to ...\n",
       "00040093b2687caa    alignment on this subject and which are contra...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['comment_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ad646cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>is_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001ea8717f6de06</th>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000247e83dcc1211</th>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002f87b16116a7f</th>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003e1cccfd5a40a</th>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00059ace3e3e9a53</th>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000663aff0fffc80</th>\n",
       "      <td>this other one from 1897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000689dd34e20979</th>\n",
       "      <td>== Reason for banning throwing == \\n\\n This ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000844b52dee5f3f</th>\n",
       "      <td>|blocked]] from editing Wikipedia.   |</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00091c35fa9d0465</th>\n",
       "      <td>== Arabs are committing genocide in Iraq, but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000968ce11f5ee34</th>\n",
       "      <td>Please stop. If you continue to vandalize Wiki...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0001ea8717f6de06  Thank you for understanding. I think very high...      0   \n",
       "000247e83dcc1211                   :Dear god this site is horrible.      0   \n",
       "0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...      0   \n",
       "0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...      0   \n",
       "00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...      0   \n",
       "000663aff0fffc80                           this other one from 1897      0   \n",
       "000689dd34e20979  == Reason for banning throwing == \\n\\n This ar...      0   \n",
       "000844b52dee5f3f             |blocked]] from editing Wikipedia.   |      0   \n",
       "00091c35fa9d0465  == Arabs are committing genocide in Iraq, but ...      1   \n",
       "000968ce11f5ee34  Please stop. If you continue to vandalize Wiki...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "id                                                                       \n",
       "0001ea8717f6de06             0        0       0       0              0   \n",
       "000247e83dcc1211             0        0       0       0              0   \n",
       "0002f87b16116a7f             0        0       0       0              0   \n",
       "0003e1cccfd5a40a             0        0       0       0              0   \n",
       "00059ace3e3e9a53             0        0       0       0              0   \n",
       "000663aff0fffc80             0        0       0       0              0   \n",
       "000689dd34e20979             0        0       0       0              0   \n",
       "000844b52dee5f3f             0        0       0       0              0   \n",
       "00091c35fa9d0465             0        0       0       0              0   \n",
       "000968ce11f5ee34             0        0       0       0              0   \n",
       "\n",
       "                  is_hate  \n",
       "id                         \n",
       "0001ea8717f6de06        0  \n",
       "000247e83dcc1211        0  \n",
       "0002f87b16116a7f        0  \n",
       "0003e1cccfd5a40a        0  \n",
       "00059ace3e3e9a53        0  \n",
       "000663aff0fffc80        0  \n",
       "000689dd34e20979        0  \n",
       "000844b52dee5f3f        0  \n",
       "00091c35fa9d0465        1  \n",
       "000968ce11f5ee34        0  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ef08e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of no/is hate for train set:  0    0.898321\n",
      "1    0.101679\n",
      "Name: is_hate, dtype: float64\n",
      "Ratio of no/is hate for test set:  0    0.90242\n",
      "1    0.09758\n",
      "Name: is_hate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# get the distribution of the labels to see if roughly similar for both\n",
    "\n",
    "is_hate_count_train = data_train['is_hate'].value_counts()\n",
    "ratio_train = is_hate_count_train/ len(data_train)\n",
    "\n",
    "is_hate_count_test = data_test['is_hate'].value_counts()\n",
    "ratio_test = is_hate_count_test/ len(data_test)\n",
    "\n",
    "print('Ratio of no/is hate for train set: ', ratio_train)\n",
    "print('Ratio of no/is hate for test set: ', ratio_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41c0af",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96650bb9",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ef87f",
   "metadata": {},
   "source": [
    "**Note**: We would need to make a loop for the different combinations of \n",
    "preprocessing (none, only stemming, only lemming, only stop word removal and every combination of this)\n",
    "Either as coloumns that can be used to iterate over for the model training and validation, or make the preprocessing\n",
    "and then go further and repeat from beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "25fb2d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\flras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\flras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\flras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\flras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def handle_negations(tokens):\n",
    "    negation_words = {'not', \"n't\", 'no', 'never', 'none'}\n",
    "    processed_tokens = []\n",
    "    skip_next = False\n",
    "\n",
    "    for i, word in enumerate(tokens):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        if word in negation_words and i + 1 < len(tokens):\n",
    "            processed_tokens.append(word + '_' + tokens[i + 1])\n",
    "            skip_next = True\n",
    "        else:\n",
    "            processed_tokens.append(word)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Assuming stop_words is defined somewhere\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts treebank tags to wordnet tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def preprocess_text(text, use_lower=False, remove_stopwords=False, use_stemming=False, use_lemming=False, combine_negations=False, keep_semantic_punctuation=False):\n",
    "    if use_lower:\n",
    "        text = text.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    if combine_negations:\n",
    "        tokens = handle_negations(tokens)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    if not keep_semantic_punctuation:\n",
    "        tokens = [re.sub(r'[^\\w\\s]', '', word) for word in tokens]\n",
    "    \n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    elif use_lemming:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_tokens]\n",
    "    \n",
    "    filtered_tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to the training and test datasets\n",
    "# We don't pass the rare_words parameter, so rare word removal is not performed\n",
    "#data_train['comment_text_clean_2'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_stemming=False))\n",
    "#data_test['comment_text_clean_2'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_stemming=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dd6594bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "aeff00b0-ba19-4319-a324-a0c540cba267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping semantic punctuation (keeping ! and ?)\n",
    "data_train['text_no_punctuation'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False))\n",
    "data_train['text_with_punctuation'] = data_train['text_no_punctuation'].apply(lambda x: preprocess_text(x, keep_semantic_punctuation=True,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5e6ab4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping semantic punctuation (keeping ! and ?)\n",
    "data_test['text_no_punctuation'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False))\n",
    "data_test['text_with_punctuation'] = data_test['text_no_punctuation'].apply(lambda x: preprocess_text(x, keep_semantic_punctuation=True,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b18892b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c3cc50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bd7a5fd7-c294-4b22-b53b-5aa973df0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep negations\n",
    "data_train['text_without_negations'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_lemming=True, combine_negations=False))\n",
    "data_train['text_with_negations'] = data_train['text_without_negations'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=False, keep_semantic_punctuation=False, use_lemming=False, combine_negations=True))\n",
    "\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "571005b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep negations\n",
    "data_test['text_without_negations'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_lemming=True, combine_negations=False))\n",
    "data_test['text_with_negations'] = data_test['text_without_negations'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=False, keep_semantic_punctuation=False, use_lemming=False, combine_negations=True))\n",
    "\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c8dc11b6-d602-4f82-9c04-b482b7eaebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Steps of Processing\n",
    "# Only lowercase\n",
    "data_train['data_text_1'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=False, keep_semantic_punctuation=True, use_stemming=False))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "69fe628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Steps of Processing\n",
    "# Only lowercase\n",
    "data_test['data_text_1'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=False, keep_semantic_punctuation=True, use_stemming=False))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6dd65387",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lowercase and stopwords removal\n",
    "data_train['data_text_2'] = data_train['data_text_1'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "42797ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lowercase and stopwords removal\n",
    "data_test['data_text_2'] = data_test['data_text_1'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70e8467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add punctuation handling\n",
    "data_train['data_text_3'] = data_train['data_text_2'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=False, keep_semantic_punctuation=True, use_stemming=False))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2bd96663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add punctuation handling\n",
    "data_test['data_text_3'] = data_test['data_text_2'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=False, keep_semantic_punctuation=True, use_stemming=False))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e308133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Incorporate lemmatization\n",
    "data_train['data_text_4'] = data_train['data_text_3'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=False, keep_semantic_punctuation=False, combine_negations=True))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bbabab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Incorporate lemmatization\n",
    "data_test['data_text_4'] = data_test['data_text_3'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=False, keep_semantic_punctuation=False, combine_negations=True))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "049def12",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Incorporate stemming or lemming, depends on which was is more efficent\n",
    "data_train['data_text_5_stemming'] = data_train['data_text_4'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=False, keep_semantic_punctuation=False, use_stemming=True, combine_negations=False))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Incorporate stemming or lemming, depends on which was is more efficent\n",
    "data_test['data_text_5_stemming'] = data_test['data_text_4'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=False, keep_semantic_punctuation=False, use_stemming=True, combine_negations=False))\n",
    "data_test.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4810fd2-8cd6-4658-b814-ea2e61792e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['data_text_5_lemming'] = data_train['data_text_4'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=False, keep_semantic_punctuation=False, use_lemming=True, combine_negations=False))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['data_text_5_lemming'] = data_test['data_text_4'].apply(lambda x: preprocess_text(x, use_lower=False, remove_stopwords=False, keep_semantic_punctuation=False, use_lemming=True, combine_negations=False))\n",
    "data_test.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "55f762d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>is_hate</th>\n",
       "      <th>text_with_punctuation</th>\n",
       "      <th>text_without_negations</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>youi</th>\n",
       "      <th>young</th>\n",
       "      <th>yourselfgo</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2015 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_text  toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           NaN    NaN           NaN      NaN     NaN     NaN            NaN   \n",
       "1           NaN    NaN           NaN      NaN     NaN     NaN            NaN   \n",
       "2           NaN    NaN           NaN      NaN     NaN     NaN            NaN   \n",
       "\n",
       "   is_hate  text_with_punctuation  text_without_negations  ...  yes  \\\n",
       "0      NaN                    NaN                     NaN  ...  0.0   \n",
       "1      NaN                    NaN                     NaN  ...  0.0   \n",
       "2      NaN                    NaN                     NaN  ...  0.0   \n",
       "\n",
       "   yesterday  yet      york  you  youi  young  yourselfgo  youtube  zero  \n",
       "0        0.0  0.0  0.352046  0.0   0.0    0.0         0.0      0.0   0.0  \n",
       "1        0.0  0.0  0.000000  0.0   0.0    0.0         0.0      0.0   0.0  \n",
       "2        0.0  0.0  0.000000  0.0   0.0    0.0         0.0      0.0   0.0  \n",
       "\n",
       "[3 rows x 2015 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"./train_all_coloumns.csv\", index_col=0)\n",
    "\n",
    "data_test = pd.read_csv(\"./test_all_coloumns.csv\", index_col=0)\n",
    "data_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c54090b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_csv('test_all_coloumns.csv')\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ab2fc-ecc6-4865-a9d6-8b4962773c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>is_hate</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "      <th>text_with_punctuation</th>\n",
       "      <th>text_without_negations</th>\n",
       "      <th>text_with_negations</th>\n",
       "      <th>data_text_1</th>\n",
       "      <th>data_text_2</th>\n",
       "      <th>data_text_3</th>\n",
       "      <th>data_text_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001ea8717f6de06</th>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thank understanding think highly would revert ...</td>\n",
       "      <td>thank understanding think highly would revert ...</td>\n",
       "      <td>thank understand think highly would revert wit...</td>\n",
       "      <td>thank understand think highly would revert wit...</td>\n",
       "      <td>thank you for understanding i think very highl...</td>\n",
       "      <td>thank understanding think highly would revert ...</td>\n",
       "      <td>thank understanding think highly would revert ...</td>\n",
       "      <td>thank understanding think highly would revert ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000247e83dcc1211</th>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dear god site horrible</td>\n",
       "      <td>dear god site horrible</td>\n",
       "      <td>dear god site horrible</td>\n",
       "      <td>dear god site horrible</td>\n",
       "      <td>dear god this site is horrible</td>\n",
       "      <td>dear god site horrible</td>\n",
       "      <td>dear god site horrible</td>\n",
       "      <td>dear god site horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002f87b16116a7f</th>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>somebody invariably try add religion really me...</td>\n",
       "      <td>somebody invariably try add religion really me...</td>\n",
       "      <td>somebody invariably try add religion really me...</td>\n",
       "      <td>somebody invariably try add religion really me...</td>\n",
       "      <td>somebody will invariably try to add religion r...</td>\n",
       "      <td>somebody invariably try add religion really me...</td>\n",
       "      <td>somebody invariably try add religion really me...</td>\n",
       "      <td>somebody invariably try add religion really me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003e1cccfd5a40a</th>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>says right type type institution needed case t...</td>\n",
       "      <td>says right type type institution needed case t...</td>\n",
       "      <td>say right type type institution need case thre...</td>\n",
       "      <td>say right type type institution need case thre...</td>\n",
       "      <td>it says it right there that it is a type the t...</td>\n",
       "      <td>says right type type institution needed case t...</td>\n",
       "      <td>says right type type institution needed case t...</td>\n",
       "      <td>says right type type institution needed case t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00059ace3e3e9a53</th>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>adding new product list make sure s relevant a...</td>\n",
       "      <td>adding new product list make sure s relevant a...</td>\n",
       "      <td>add new product list make sure s relevant add ...</td>\n",
       "      <td>add new product list make sure s relevant add ...</td>\n",
       "      <td>before adding a new product to the list make s...</td>\n",
       "      <td>adding new product list make sure relevant add...</td>\n",
       "      <td>adding new product list make sure relevant add...</td>\n",
       "      <td>adding new product list make sure relevant add...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0001ea8717f6de06  Thank you for understanding. I think very high...      0   \n",
       "000247e83dcc1211                   :Dear god this site is horrible.      0   \n",
       "0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...      0   \n",
       "0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...      0   \n",
       "00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "id                                                                       \n",
       "0001ea8717f6de06             0        0       0       0              0   \n",
       "000247e83dcc1211             0        0       0       0              0   \n",
       "0002f87b16116a7f             0        0       0       0              0   \n",
       "0003e1cccfd5a40a             0        0       0       0              0   \n",
       "00059ace3e3e9a53             0        0       0       0              0   \n",
       "\n",
       "                  is_hate                                text_no_punctuation  \\\n",
       "id                                                                             \n",
       "0001ea8717f6de06        0  thank understanding think highly would revert ...   \n",
       "000247e83dcc1211        0                             dear god site horrible   \n",
       "0002f87b16116a7f        0  somebody invariably try add religion really me...   \n",
       "0003e1cccfd5a40a        0  says right type type institution needed case t...   \n",
       "00059ace3e3e9a53        0  adding new product list make sure s relevant a...   \n",
       "\n",
       "                                              text_with_punctuation  \\\n",
       "id                                                                    \n",
       "0001ea8717f6de06  thank understanding think highly would revert ...   \n",
       "000247e83dcc1211                             dear god site horrible   \n",
       "0002f87b16116a7f  somebody invariably try add religion really me...   \n",
       "0003e1cccfd5a40a  says right type type institution needed case t...   \n",
       "00059ace3e3e9a53  adding new product list make sure s relevant a...   \n",
       "\n",
       "                                             text_without_negations  \\\n",
       "id                                                                    \n",
       "0001ea8717f6de06  thank understand think highly would revert wit...   \n",
       "000247e83dcc1211                             dear god site horrible   \n",
       "0002f87b16116a7f  somebody invariably try add religion really me...   \n",
       "0003e1cccfd5a40a  say right type type institution need case thre...   \n",
       "00059ace3e3e9a53  add new product list make sure s relevant add ...   \n",
       "\n",
       "                                                text_with_negations  \\\n",
       "id                                                                    \n",
       "0001ea8717f6de06  thank understand think highly would revert wit...   \n",
       "000247e83dcc1211                             dear god site horrible   \n",
       "0002f87b16116a7f  somebody invariably try add religion really me...   \n",
       "0003e1cccfd5a40a  say right type type institution need case thre...   \n",
       "00059ace3e3e9a53  add new product list make sure s relevant add ...   \n",
       "\n",
       "                                                        data_text_1  \\\n",
       "id                                                                    \n",
       "0001ea8717f6de06  thank you for understanding i think very highl...   \n",
       "000247e83dcc1211                     dear god this site is horrible   \n",
       "0002f87b16116a7f  somebody will invariably try to add religion r...   \n",
       "0003e1cccfd5a40a  it says it right there that it is a type the t...   \n",
       "00059ace3e3e9a53  before adding a new product to the list make s...   \n",
       "\n",
       "                                                        data_text_2  \\\n",
       "id                                                                    \n",
       "0001ea8717f6de06  thank understanding think highly would revert ...   \n",
       "000247e83dcc1211                             dear god site horrible   \n",
       "0002f87b16116a7f  somebody invariably try add religion really me...   \n",
       "0003e1cccfd5a40a  says right type type institution needed case t...   \n",
       "00059ace3e3e9a53  adding new product list make sure relevant add...   \n",
       "\n",
       "                                                        data_text_3  \\\n",
       "id                                                                    \n",
       "0001ea8717f6de06  thank understanding think highly would revert ...   \n",
       "000247e83dcc1211                             dear god site horrible   \n",
       "0002f87b16116a7f  somebody invariably try add religion really me...   \n",
       "0003e1cccfd5a40a  says right type type institution needed case t...   \n",
       "00059ace3e3e9a53  adding new product list make sure relevant add...   \n",
       "\n",
       "                                                        data_text_4  \n",
       "id                                                                   \n",
       "0001ea8717f6de06  thank understanding think highly would revert ...  \n",
       "000247e83dcc1211                             dear god site horrible  \n",
       "0002f87b16116a7f  somebody invariably try add religion really me...  \n",
       "0003e1cccfd5a40a  says right type type institution needed case t...  \n",
       "00059ace3e3e9a53  adding new product list make sure relevant add...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "35bf8195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_text              0\n",
      "toxic                     0\n",
      "severe_toxic              0\n",
      "obscene                   0\n",
      "threat                    0\n",
      "insult                    0\n",
      "identity_hate             0\n",
      "is_hate                   0\n",
      "text_no_punctuation       0\n",
      "text_with_punctuation     0\n",
      "text_without_negations    0\n",
      "text_with_negations       0\n",
      "data_text_1               0\n",
      "data_text_2               0\n",
      "data_text_3               0\n",
      "data_text_4               0\n",
      "dtype: int64\n",
      "comment_text              0\n",
      "toxic                     0\n",
      "severe_toxic              0\n",
      "obscene                   0\n",
      "threat                    0\n",
      "insult                    0\n",
      "identity_hate             0\n",
      "is_hate                   0\n",
      "text_no_punctuation       0\n",
      "text_with_punctuation     0\n",
      "text_without_negations    0\n",
      "text_with_negations       0\n",
      "data_text_1               0\n",
      "data_text_2               0\n",
      "data_text_3               0\n",
      "data_text_4               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data_train.isna().sum())\n",
    "print(data_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0f885433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>is_hate</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "      <th>text_with_punctuation</th>\n",
       "      <th>text_without_negations</th>\n",
       "      <th>text_with_negations</th>\n",
       "      <th>data_text_1</th>\n",
       "      <th>data_text_2</th>\n",
       "      <th>data_text_3</th>\n",
       "      <th>data_text_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000997932d777bf</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>explanation edits make username hardcore metal...</td>\n",
       "      <td>explanation edits make username hardcore metal...</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000103f0d9cfb60f</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daww matches background colour m seemingly stu...</td>\n",
       "      <td>daww matches background colour m seemingly stu...</td>\n",
       "      <td>daww match background colour m seemingly stuck...</td>\n",
       "      <td>daww match background colour m seemingly stuck...</td>\n",
       "      <td>he matches this background colour i seemingly ...</td>\n",
       "      <td>matches background colour seemingly stuck than...</td>\n",
       "      <td>matches background colour seemingly stuck than...</td>\n",
       "      <td>matches background colour seemingly stuck than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000113f07ec002fd</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man m really trying edit war s guy constan...</td>\n",
       "      <td>hey man m really trying edit war s guy constan...</td>\n",
       "      <td>hey man m really try edit war s guy constantly...</td>\n",
       "      <td>hey man m really try edit war s guy constantly...</td>\n",
       "      <td>hey man i really not trying to edit war it jus...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001b41b1c6bb37e</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ca nt make real suggestions improvement wonder...</td>\n",
       "      <td>ca nt make real suggestions improvement wonder...</td>\n",
       "      <td>ca nt make real suggestion improvement wonder ...</td>\n",
       "      <td>ca nt make real suggestion improvement wonder ...</td>\n",
       "      <td>more i ca make any real suggestions on improve...</td>\n",
       "      <td>ca make real suggestions improvement wondered ...</td>\n",
       "      <td>ca make real suggestions improvement wondered ...</td>\n",
       "      <td>ca make real suggestions improvement wondered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001d958c54c6e35</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sir hero chance remember page s</td>\n",
       "      <td>sir hero chance remember page s</td>\n",
       "      <td>sir hero chance remember page s</td>\n",
       "      <td>sir hero chance remember page s</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "id                                                                       \n",
       "0000997932d777bf             0        0       0       0              0   \n",
       "000103f0d9cfb60f             0        0       0       0              0   \n",
       "000113f07ec002fd             0        0       0       0              0   \n",
       "0001b41b1c6bb37e             0        0       0       0              0   \n",
       "0001d958c54c6e35             0        0       0       0              0   \n",
       "\n",
       "                  is_hate                                text_no_punctuation  \\\n",
       "id                                                                             \n",
       "0000997932d777bf        0  explanation edits made username hardcore metal...   \n",
       "000103f0d9cfb60f        0  daww matches background colour m seemingly stu...   \n",
       "000113f07ec002fd        0  hey man m really trying edit war s guy constan...   \n",
       "0001b41b1c6bb37e        0  ca nt make real suggestions improvement wonder...   \n",
       "0001d958c54c6e35        0                    sir hero chance remember page s   \n",
       "\n",
       "                                              text_with_punctuation  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "000103f0d9cfb60f  daww matches background colour m seemingly stu...   \n",
       "000113f07ec002fd  hey man m really trying edit war s guy constan...   \n",
       "0001b41b1c6bb37e  ca nt make real suggestions improvement wonder...   \n",
       "0001d958c54c6e35                    sir hero chance remember page s   \n",
       "\n",
       "                                             text_without_negations  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation edits make username hardcore metal...   \n",
       "000103f0d9cfb60f  daww match background colour m seemingly stuck...   \n",
       "000113f07ec002fd  hey man m really try edit war s guy constantly...   \n",
       "0001b41b1c6bb37e  ca nt make real suggestion improvement wonder ...   \n",
       "0001d958c54c6e35                    sir hero chance remember page s   \n",
       "\n",
       "                                                text_with_negations  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation edits make username hardcore metal...   \n",
       "000103f0d9cfb60f  daww match background colour m seemingly stuck...   \n",
       "000113f07ec002fd  hey man m really try edit war s guy constantly...   \n",
       "0001b41b1c6bb37e  ca nt make real suggestion improvement wonder ...   \n",
       "0001d958c54c6e35                    sir hero chance remember page s   \n",
       "\n",
       "                                                        data_text_1  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation why the edits made under my userna...   \n",
       "000103f0d9cfb60f  he matches this background colour i seemingly ...   \n",
       "000113f07ec002fd  hey man i really not trying to edit war it jus...   \n",
       "0001b41b1c6bb37e  more i ca make any real suggestions on improve...   \n",
       "0001d958c54c6e35  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                                        data_text_2  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "000103f0d9cfb60f  matches background colour seemingly stuck than...   \n",
       "000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "0001b41b1c6bb37e  ca make real suggestions improvement wondered ...   \n",
       "0001d958c54c6e35                      sir hero chance remember page   \n",
       "\n",
       "                                                        data_text_3  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "000103f0d9cfb60f  matches background colour seemingly stuck than...   \n",
       "000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "0001b41b1c6bb37e  ca make real suggestions improvement wondered ...   \n",
       "0001d958c54c6e35                      sir hero chance remember page   \n",
       "\n",
       "                                                        data_text_4  \n",
       "id                                                                   \n",
       "0000997932d777bf  explanation edits made username hardcore metal...  \n",
       "000103f0d9cfb60f  matches background colour seemingly stuck than...  \n",
       "000113f07ec002fd  hey man really trying edit war guy constantly ...  \n",
       "0001b41b1c6bb37e  ca make real suggestions improvement wondered ...  \n",
       "0001d958c54c6e35                      sir hero chance remember page  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2580c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test set undersample test set\n",
    "# Define test and train set\n",
    "def datasetDefinition(columnName):\n",
    "    X_train = data_train[columnName]\n",
    "    y_train = data_train[\"is_hate\"]\n",
    "\n",
    "    X_test = data_test[columnName]\n",
    "    y_test = data_test[\"is_hate\"]\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f8f7c",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2c013",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Notes**: Tokenizing with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dafd0489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9e3219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63978, 16)\n",
      "(63720, 16)\n"
     ]
    }
   ],
   "source": [
    "print(data_test.shape)\n",
    "data_test.dropna(inplace=True)\n",
    "print(data_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "49d0727c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63720,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['text_no_punctuation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d456ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def makeToTensors(X_train, y_train, X_test, y_test):\n",
    "    # Make the test and train sets to tensors and apply TF-IDF\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Transform testing data\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # Convert TF-IDF matrices to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_tensor, y_train_tensor, test_size=0.8, stratify=y_train, random_state=42)\n",
    "    # Initialize RandomUnderSampler\n",
    "    undersampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "    # Resample the data\n",
    "    X_resampled, y_resampled = undersampler.fit_resample(X_test_tensor, y_test_tensor)\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b6a87",
   "metadata": {},
   "source": [
    "## Model Implementation & Test with Testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baace6c",
   "metadata": {},
   "source": [
    "**Note**: Does a CNN makes sense for sentiment analysis? or a simpler model?\n",
    "\n",
    "**Answers and additional Notes**:\n",
    "Make a CNN with PyTorch using skorch as wrapper to make it possible to use sklearn.pipeline with the model\n",
    "This way gridsearch for hyper parameters is possible and tfidfVectorizer can be used for tf-idf\n",
    "CNN: vector size 300, conv. layer of some size, flatten, relu, end with softmax or something\n",
    "Example: https://www.kaggle.com/code/raviusz/jigsaw-toxic-comment\n",
    "example look very good to get basics and then change some of architecture\n",
    "hyperparameter tuning for each model? only if time permits, alt. tune on best model and use for rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ff84f",
   "metadata": {},
   "source": [
    "**Note**: We will use the given test set to compare the different approaches. Make a dataframe with all the results\n",
    "in accuracy, f1, recall, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "970d9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CNN: The basic model\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=5)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv_output_size = self._get_conv_output_size(2000)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.conv_output_size, 64)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "    def _get_conv_output_size(self, input_size):\n",
    "        x = torch.randn(1, 1, input_size)  # Add channel dimension\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "\n",
    "batchSize = 25\n",
    "\n",
    "def trainCNN(X_train, y_train, X_test, y_test,  batch_size=batchSize, epochs=7, learning_rate=0.001):\n",
    "\n",
    "    # Step 4: Train the model\n",
    "    train_dataset = TensorDataset(X_train, y_train)  # Assuming X_train_tensor and y_train_tensor are tensors\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    # Move model to GPU\n",
    "    model = CNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(f'learning rate: {learning_rate}, total number of epochs: {epochs}')\n",
    "    for epoch in range(int(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = total_loss / len(train_dataset)\n",
    "        epoch_accuracy = correct / total\n",
    "        train_loss.append(epoch_loss)\n",
    "        train_accuracy.append(epoch_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss}, Accuracy: {epoch_accuracy}')\n",
    "\n",
    "    # Step 5: Evaluate the model\n",
    "    # Assuming X_test_tensor and y_test_tensor are tensors\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_predicted = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += torch.sum(predicted == labels).item()\n",
    "            total_predicted += len(predicted)\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = total_correct / total_predicted\n",
    "    \n",
    "    print(f'Test Accuracy: {accuracy}')\n",
    "    return accuracy, model, train_loss, train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fea5efae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.0001, total number of epochs: 30\n",
      "Epoch [1/30], Loss: 0.31321185253870576, Accuracy: 0.8985711599924798\n",
      "Epoch [2/30], Loss: 0.23580141776741137, Accuracy: 0.9188443943097073\n",
      "Epoch [3/30], Loss: 0.21611015361300612, Accuracy: 0.9271792943535753\n",
      "Epoch [4/30], Loss: 0.20212608106894142, Accuracy: 0.9313780785862004\n",
      "Epoch [5/30], Loss: 0.19138304286623514, Accuracy: 0.9357021996615905\n",
      "Epoch [6/30], Loss: 0.18256658843851598, Accuracy: 0.93899229178417\n",
      "Epoch [7/30], Loss: 0.1744304354340347, Accuracy: 0.941561697060851\n",
      "Epoch [8/30], Loss: 0.1668829640260127, Accuracy: 0.9439117628626935\n",
      "Epoch [9/30], Loss: 0.16172726659334535, Accuracy: 0.9466378391928307\n",
      "Epoch [10/30], Loss: 0.15508793214117286, Accuracy: 0.9483612207808485\n",
      "Epoch [11/30], Loss: 0.14817559423022933, Accuracy: 0.9508052892147647\n",
      "Epoch [12/30], Loss: 0.14193019108316718, Accuracy: 0.9518706523782666\n",
      "Epoch [13/30], Loss: 0.13660416427097186, Accuracy: 0.9538760418625055\n",
      "Epoch [14/30], Loss: 0.12994292096519153, Accuracy: 0.9550667418687723\n",
      "Epoch [15/30], Loss: 0.1248167290206899, Accuracy: 0.9577928181989096\n",
      "Epoch [16/30], Loss: 0.12106155246130722, Accuracy: 0.9584508366234255\n",
      "Epoch [17/30], Loss: 0.11726043553492783, Accuracy: 0.9597042050510748\n",
      "Epoch [18/30], Loss: 0.11066493736069909, Accuracy: 0.9611455787428714\n",
      "Epoch [19/30], Loss: 0.10618139628124885, Accuracy: 0.9630882998057279\n",
      "Epoch [20/30], Loss: 0.10139036273024113, Accuracy: 0.9646236761295983\n",
      "Epoch [21/30], Loss: 0.0984079994493862, Accuracy: 0.9640909945478473\n",
      "Epoch [22/30], Loss: 0.09258743254642793, Accuracy: 0.9667230682459109\n",
      "Epoch [23/30], Loss: 0.0904319641962742, Accuracy: 0.967537757723883\n",
      "Epoch [24/30], Loss: 0.08761685404543827, Accuracy: 0.9671304129848969\n",
      "Epoch [25/30], Loss: 0.081701403537597, Accuracy: 0.9698878235257253\n",
      "Epoch [26/30], Loss: 0.07998583486005065, Accuracy: 0.97076518142508\n",
      "Epoch [27/30], Loss: 0.07721872644160371, Accuracy: 0.9702638340540202\n",
      "Epoch [28/30], Loss: 0.07242554423742292, Accuracy: 0.9714231998495958\n"
     ]
    }
   ],
   "source": [
    "# for all the columns load them into X_train\n",
    "# make the tensors, train the cnn and thenn take the outcome and add it to a df\n",
    "columnName = ['text_no_punctuation', 'text_with_punctuation', 'text_without_negations', 'text_with_negations', \n",
    "              'data_text_1', 'data_text_2', 'data_text_3', 'data_text_4']\n",
    "\n",
    "results = []\n",
    "all_train_loss = []\n",
    "all_train_accuracy = []\n",
    "for name in columnName:\n",
    "    X_train, y_train, X_test, y_test = datasetDefinition(name)\n",
    "    X_train_T, y_train_T, X_test_T, y_test_T = makeToTensors(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Get the test accuracy using the best model\n",
    "    accuracy, model, train_loss, train_accuracy = trainCNN(X_train_T, y_train_T, X_test_T, y_test_T, epochs=30, learning_rate=0.0001)\n",
    "    \n",
    "    all_train_loss.append(train_loss)\n",
    "    all_train_accuracy.append(train_accuracy)\n",
    "\n",
    "    plt.plot(train_loss, label=f'{name} Loss')\n",
    "    plt.plot(train_accuracy, label=f'{name} Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Loss and Accuracy vs. Epochs for {name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    results.append({'Column': name, 'Test_Accuracy': accuracy})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('hyperparameter_tuning_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905d948",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16300/477429924.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumnName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_train_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'{name} Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epochs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "columnName = ['text_lemmatization', 'text_punctuation', 'text_no_punctuation', 'text_negations', 'text_no_negations', 'data_text_1', 'data_text_2',\n",
    "            'data_text_2', 'data_text_3', 'data_text_4']\n",
    "\n",
    "for i, name in enumerate(columnName):\n",
    "    plt.plot(all_train_loss[i], label=f'{name} Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs. Epochs for Different Preprocessing Methods')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i, name in enumerate(columnName):\n",
    "    plt.plot(all_train_accuracy[i], label=f'{name} Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy vs. Epochs for Different Preprocessing Methods')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef2092",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text_no_punctuation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_no_punctuation'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6400/2189842750.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumnName\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasetDefinition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mlinear_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6400/4109458893.py\u001b[0m in \u001b[0;36mdatasetDefinition\u001b[1;34m(columnName)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Define test and train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdatasetDefinition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumnName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumnName\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"is_hate\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_no_punctuation'"
     ]
    }
   ],
   "source": [
    "\n",
    "columnName = ['text_no_punctuation', 'text_with_punctuation', 'text_without_negations', \n",
    "              'text_with_negations', 'data_text_1', 'data_text_2', 'data_text_3', 'data_text_4']\n",
    "\n",
    "metrics_dict = {'accuracy': [], 'f1-score': [], 'recall': []}\n",
    "linear_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(max_features=1000),\n",
    "    LogisticRegression(solver='sag', max_iter=1000)\n",
    ")\n",
    "for name in columnName:\n",
    "    X_train, y_train, X_test, y_test = datasetDefinition(name)\n",
    "    linear_pipeline.fit(X_train, y_train)\n",
    "    y_pred = linear_pipeline.predict(X_test)\n",
    "    print(name, classification_report(y_test, y_pred))\n",
    "    metrics_dict['accuracy'].append(classification_report['accuracy_score'])\n",
    "    metrics_dict['f1-score'].append(classification_report['macro avg']['f1-score'])\n",
    "    metrics_dict['recall'].append(classification_report['macro avg']['recall'])\n",
    "    \n",
    "\n",
    "for metric in metrics_dict.keys():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(columnName, metrics_dict[metric], color='skyblue')\n",
    "    plt.title(f'{metric.capitalize()} for each Column')\n",
    "    plt.xlabel('Column Name')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
