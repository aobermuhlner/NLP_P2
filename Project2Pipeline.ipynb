{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d484bd",
   "metadata": {},
   "source": [
    "# Project 2 NLP: Hatespeech Classifier\n",
    "\n",
    "## Authors:\n",
    "\n",
    "Adrian Obermühlner & Freja Rasmussen\n",
    "\n",
    "## Resarch Question:\n",
    "\n",
    "How do different preprocessing methods (nothing, stop word removal, lemming, stemming,…) affect the result of a hate speech classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712926f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71084f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Preprocessing imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizing\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7be0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b6cdf",
   "metadata": {},
   "source": [
    "## Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198a399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BINARY_LABEL = \"is_hate\"\n",
    "CATEGORIES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "np.random.seed(RANDOM_SEED)  # set random seed for reproducibility\n",
    "# Make the labels into hate and no hate as 1 and 0\n",
    "\n",
    "def binarize_labels(df):\n",
    "    return (df[CATEGORIES].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "data_train = pd.read_csv(\"./data/train/train.csv\", index_col=0)\n",
    "data_train[BINARY_LABEL] = binarize_labels(data_train)\n",
    "\n",
    "data_test = pd.read_csv(\"./data/test/test.csv\", index_col=0).join(\n",
    "    pd.read_csv(\"./data/test_labels/test_labels.csv\", index_col=0)\n",
    ")\n",
    "data_test.drop(data_test[data_test[\"toxic\"] == -1].index, inplace=True)\n",
    "data_test[BINARY_LABEL] = binarize_labels(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb59a0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0000997932d777bf    Explanation\\nWhy the edits made under my usern...\n",
       "000103f0d9cfb60f    D'aww! He matches this background colour I'm s...\n",
       "000113f07ec002fd    Hey man, I'm really not trying to edit war. It...\n",
       "0001b41b1c6bb37e    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "0001d958c54c6e35    You, sir, are my hero. Any chance you remember...\n",
       "00025465d4725e87    \"\\n\\nCongratulations from me as well, use the ...\n",
       "0002bcb3da6cb337         COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
       "00031b1e95af7921    Your vandalism to the Matt Shirvington article...\n",
       "00037261f536c51d    Sorry if the word 'nonsense' was offensive to ...\n",
       "00040093b2687caa    alignment on this subject and which are contra...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['comment_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad646cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>is_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001ea8717f6de06</th>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000247e83dcc1211</th>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002f87b16116a7f</th>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003e1cccfd5a40a</th>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00059ace3e3e9a53</th>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000663aff0fffc80</th>\n",
       "      <td>this other one from 1897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000689dd34e20979</th>\n",
       "      <td>== Reason for banning throwing == \\n\\n This ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000844b52dee5f3f</th>\n",
       "      <td>|blocked]] from editing Wikipedia.   |</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00091c35fa9d0465</th>\n",
       "      <td>== Arabs are committing genocide in Iraq, but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000968ce11f5ee34</th>\n",
       "      <td>Please stop. If you continue to vandalize Wiki...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0001ea8717f6de06  Thank you for understanding. I think very high...      0   \n",
       "000247e83dcc1211                   :Dear god this site is horrible.      0   \n",
       "0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...      0   \n",
       "0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...      0   \n",
       "00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...      0   \n",
       "000663aff0fffc80                           this other one from 1897      0   \n",
       "000689dd34e20979  == Reason for banning throwing == \\n\\n This ar...      0   \n",
       "000844b52dee5f3f             |blocked]] from editing Wikipedia.   |      0   \n",
       "00091c35fa9d0465  == Arabs are committing genocide in Iraq, but ...      1   \n",
       "000968ce11f5ee34  Please stop. If you continue to vandalize Wiki...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "id                                                                       \n",
       "0001ea8717f6de06             0        0       0       0              0   \n",
       "000247e83dcc1211             0        0       0       0              0   \n",
       "0002f87b16116a7f             0        0       0       0              0   \n",
       "0003e1cccfd5a40a             0        0       0       0              0   \n",
       "00059ace3e3e9a53             0        0       0       0              0   \n",
       "000663aff0fffc80             0        0       0       0              0   \n",
       "000689dd34e20979             0        0       0       0              0   \n",
       "000844b52dee5f3f             0        0       0       0              0   \n",
       "00091c35fa9d0465             0        0       0       0              0   \n",
       "000968ce11f5ee34             0        0       0       0              0   \n",
       "\n",
       "                  is_hate  \n",
       "id                         \n",
       "0001ea8717f6de06        0  \n",
       "000247e83dcc1211        0  \n",
       "0002f87b16116a7f        0  \n",
       "0003e1cccfd5a40a        0  \n",
       "00059ace3e3e9a53        0  \n",
       "000663aff0fffc80        0  \n",
       "000689dd34e20979        0  \n",
       "000844b52dee5f3f        0  \n",
       "00091c35fa9d0465        1  \n",
       "000968ce11f5ee34        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef08e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of no/is hate for train set:  0    0.898321\n",
      "1    0.101679\n",
      "Name: is_hate, dtype: float64\n",
      "Ratio of no/is hate for test set:  0    0.90242\n",
      "1    0.09758\n",
      "Name: is_hate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# get the distribution of the labels to see if roughly similar for both\n",
    "\n",
    "is_hate_count_train = data_train['is_hate'].value_counts()\n",
    "ratio_train = is_hate_count_train/ len(data_train)\n",
    "\n",
    "is_hate_count_test = data_test['is_hate'].value_counts()\n",
    "ratio_test = is_hate_count_test/ len(data_test)\n",
    "\n",
    "print('Ratio of no/is hate for train set: ', ratio_train)\n",
    "print('Ratio of no/is hate for test set: ', ratio_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41c0af",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96650bb9",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ef87f",
   "metadata": {},
   "source": [
    "**Note**: We would need to make a loop for the different combinations of \n",
    "preprocessing (none, only stemming, only lemming, only stop word removal and every combination of this)\n",
    "Either as coloumns that can be used to iterate over for the model training and validation, or make the preprocessing\n",
    "and then go further and repeat from beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25fb2d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\flras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\flras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\flras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Lowercase all text    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and apply stemming\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemming of words\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens if word.isalpha()]\n",
    "    \n",
    "    # Join the stemmed words back into a sentence\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "data_train['comment_text_clean'] = data_train['comment_text'].apply(preprocess_text)\n",
    "data_test['comment_text_clean'] = data_test['comment_text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2580c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test set\n",
    "X_train = data_train[\"comment_text_clean\"]\n",
    "y_train = data_train[\"is_hate\"]\n",
    "\n",
    "X_test = data_test[\"comment_text_clean\"]\n",
    "y_test = data_test[\"is_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ac886fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n"
     ]
    }
   ],
   "source": [
    "maxLen = 0\n",
    "\n",
    "for comment in data_train['comment_text_clean']:\n",
    "    length = len(comment.split())\n",
    "    if(length > maxLen):\n",
    "        maxLen = length\n",
    "\n",
    "print(maxLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f8f7c",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2c013",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Notes**: Tokenizing with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dafd0489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d456ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Convert TF-IDF matrices to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b6a87",
   "metadata": {},
   "source": [
    "## Model Implementation & Test with Testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baace6c",
   "metadata": {},
   "source": [
    "**Note**: Does a CNN makes sense for sentiment analysis? or a simpler model?\n",
    "\n",
    "**Answers and additional Notes**:\n",
    "Make a CNN with PyTorch using skorch as wrapper to make it possible to use sklearn.pipeline with the model\n",
    "This way gridsearch for hyper parameters is possible and tfidfVectorizer can be used for tf-idf\n",
    "CNN: vector size 300, conv. layer of some size, flatten, relu, end with softmax or something\n",
    "Example: https://www.kaggle.com/code/raviusz/jigsaw-toxic-comment\n",
    "example look very good to get basics and then change some of architecture\n",
    "hyperparameter tuning for each model? only if time permits, alt. tune on best model and use for rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ff84f",
   "metadata": {},
   "source": [
    "**Note**: We will use the given test set to compare the different approaches. Make a dataframe with all the results\n",
    "in accuracy, f1, recall, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "970d9db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 0.41500277149124526\n",
      "Epoch [2/7], Loss: 0.414940755776564\n",
      "Epoch [3/7], Loss: 0.4149407556786989\n",
      "Epoch [4/7], Loss: 0.414940755716052\n",
      "Epoch [5/7], Loss: 0.41494075609798703\n",
      "Epoch [6/7], Loss: 0.4149407555104234\n",
      "Epoch [7/7], Loss: 0.4149407559700528\n",
      "Test Accuracy: 0.90241958173122\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CNN: The basic model\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=5)\n",
    "\n",
    "        self.conv_output_size = self._get_conv_output_size(input_size)\n",
    "        self.fc1 = nn.Linear(self.conv_output_size, 128)  # Adjust input size based on output of convolutions\n",
    "        self.fc2 = nn.Linear(128, 2)  # Assuming binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def _get_conv_output_size(self, input_size):\n",
    "        x = torch.randn(1, 1, input_size)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "\n",
    "batch_size = 25\n",
    "# Step 4: Train the model\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)  # Assuming X_train_tensor and y_train_tensor are tensors\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Move model to GPU\n",
    "model = CNN(1500).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 7\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = total_loss / len(train_dataset)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss}')\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "# Assuming X_test_tensor and y_test_tensor are tensors\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
