{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d484bd",
   "metadata": {},
   "source": [
    "# Project 2 NLP: Hatespeech Classifier\n",
    "\n",
    "## Authors:\n",
    "\n",
    "Adrian Obermühlner & Freja Rasmussen\n",
    "\n",
    "## Resarch Question:\n",
    "\n",
    "How do different preprocessing methods (nothing, stop word removal, lemming, stemming,…) affect the result of a hate speech classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712926f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71084f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import regex as re\n",
    "\n",
    "# Preprocessing imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizing\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7be0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b6cdf",
   "metadata": {},
   "source": [
    "## Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198a399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BINARY_LABEL = \"is_hate\"\n",
    "CATEGORIES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "np.random.seed(RANDOM_SEED)  # set random seed for reproducibility\n",
    "# Make the labels into hate and no hate as 1 and 0\n",
    "\n",
    "def binarize_labels(df):\n",
    "    return (df[CATEGORIES].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "data_train = pd.read_csv(\"./data/train/train.csv\", index_col=0)\n",
    "data_train[BINARY_LABEL] = binarize_labels(data_train)\n",
    "\n",
    "data_test = pd.read_csv(\"./data/test/test.csv\", index_col=0).join(\n",
    "    pd.read_csv(\"./data/test_labels/test_labels.csv\", index_col=0)\n",
    ")\n",
    "data_test.drop(data_test[data_test[\"toxic\"] == -1].index, inplace=True)\n",
    "data_test[BINARY_LABEL] = binarize_labels(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb59a0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0000997932d777bf    Explanation\\r\\nWhy the edits made under my use...\n",
       "000103f0d9cfb60f    D'aww! He matches this background colour I'm s...\n",
       "000113f07ec002fd    Hey man, I'm really not trying to edit war. It...\n",
       "0001b41b1c6bb37e    \"\\r\\nMore\\r\\nI can't make any real suggestions...\n",
       "0001d958c54c6e35    You, sir, are my hero. Any chance you remember...\n",
       "00025465d4725e87    \"\\r\\n\\r\\nCongratulations from me as well, use ...\n",
       "0002bcb3da6cb337         COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
       "00031b1e95af7921    Your vandalism to the Matt Shirvington article...\n",
       "00037261f536c51d    Sorry if the word 'nonsense' was offensive to ...\n",
       "00040093b2687caa    alignment on this subject and which are contra...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['comment_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad646cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>is_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001ea8717f6de06</th>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000247e83dcc1211</th>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002f87b16116a7f</th>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003e1cccfd5a40a</th>\n",
       "      <td>\" \\r\\n\\r\\n It says it right there that it IS a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00059ace3e3e9a53</th>\n",
       "      <td>\" \\r\\n\\r\\n == Before adding a new product to t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000663aff0fffc80</th>\n",
       "      <td>this other one from 1897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000689dd34e20979</th>\n",
       "      <td>== Reason for banning throwing == \\r\\n\\r\\n Thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000844b52dee5f3f</th>\n",
       "      <td>|blocked]] from editing Wikipedia.   |</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00091c35fa9d0465</th>\n",
       "      <td>== Arabs are committing genocide in Iraq, but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000968ce11f5ee34</th>\n",
       "      <td>Please stop. If you continue to vandalize Wiki...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0001ea8717f6de06  Thank you for understanding. I think very high...      0   \n",
       "000247e83dcc1211                   :Dear god this site is horrible.      0   \n",
       "0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...      0   \n",
       "0003e1cccfd5a40a  \" \\r\\n\\r\\n It says it right there that it IS a...      0   \n",
       "00059ace3e3e9a53  \" \\r\\n\\r\\n == Before adding a new product to t...      0   \n",
       "000663aff0fffc80                           this other one from 1897      0   \n",
       "000689dd34e20979  == Reason for banning throwing == \\r\\n\\r\\n Thi...      0   \n",
       "000844b52dee5f3f             |blocked]] from editing Wikipedia.   |      0   \n",
       "00091c35fa9d0465  == Arabs are committing genocide in Iraq, but ...      1   \n",
       "000968ce11f5ee34  Please stop. If you continue to vandalize Wiki...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "id                                                                       \n",
       "0001ea8717f6de06             0        0       0       0              0   \n",
       "000247e83dcc1211             0        0       0       0              0   \n",
       "0002f87b16116a7f             0        0       0       0              0   \n",
       "0003e1cccfd5a40a             0        0       0       0              0   \n",
       "00059ace3e3e9a53             0        0       0       0              0   \n",
       "000663aff0fffc80             0        0       0       0              0   \n",
       "000689dd34e20979             0        0       0       0              0   \n",
       "000844b52dee5f3f             0        0       0       0              0   \n",
       "00091c35fa9d0465             0        0       0       0              0   \n",
       "000968ce11f5ee34             0        0       0       0              0   \n",
       "\n",
       "                  is_hate  \n",
       "id                         \n",
       "0001ea8717f6de06        0  \n",
       "000247e83dcc1211        0  \n",
       "0002f87b16116a7f        0  \n",
       "0003e1cccfd5a40a        0  \n",
       "00059ace3e3e9a53        0  \n",
       "000663aff0fffc80        0  \n",
       "000689dd34e20979        0  \n",
       "000844b52dee5f3f        0  \n",
       "00091c35fa9d0465        1  \n",
       "000968ce11f5ee34        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef08e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of no/is hate for train set:  0    0.898321\n",
      "1    0.101679\n",
      "Name: is_hate, dtype: float64\n",
      "Ratio of no/is hate for test set:  0    0.90242\n",
      "1    0.09758\n",
      "Name: is_hate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# get the distribution of the labels to see if roughly similar for both\n",
    "\n",
    "is_hate_count_train = data_train['is_hate'].value_counts()\n",
    "ratio_train = is_hate_count_train/ len(data_train)\n",
    "\n",
    "is_hate_count_test = data_test['is_hate'].value_counts()\n",
    "ratio_test = is_hate_count_test/ len(data_test)\n",
    "\n",
    "print('Ratio of no/is hate for train set: ', ratio_train)\n",
    "print('Ratio of no/is hate for test set: ', ratio_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41c0af",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96650bb9",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ef87f",
   "metadata": {},
   "source": [
    "**Note**: We would need to make a loop for the different combinations of \n",
    "preprocessing (none, only stemming, only lemming, only stop word removal and every combination of this)\n",
    "Either as coloumns that can be used to iterate over for the model training and validation, or make the preprocessing\n",
    "and then go further and repeat from beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25fb2d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def handle_negations(tokens):\n",
    "    negation_words = {'not', \"n't\", 'no', 'never', 'none'}\n",
    "    processed_tokens = []\n",
    "    skip_next = False\n",
    "\n",
    "    for i, word in enumerate(tokens):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        if word in negation_words and i + 1 < len(tokens):\n",
    "            processed_tokens.append(word + '_' + tokens[i + 1])\n",
    "            skip_next = True\n",
    "        else:\n",
    "            processed_tokens.append(word)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Assuming stop_words is defined somewhere\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts treebank tags to wordnet tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def preprocess_text(text, use_lower=True, remove_stopwords=False, use_stemming=False, combine_negations=False, keep_semantic_punctuation=True, rare_words=None):\n",
    "    if use_lower:\n",
    "        text = text.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    if combine_negations:\n",
    "        tokens = handle_negations(tokens)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    if not keep_semantic_punctuation:\n",
    "        tokens = [re.sub(r'[^\\w\\s]', '', word) for word in tokens]\n",
    "    \n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    else:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_tokens]\n",
    "    \n",
    "    filtered_tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to the training and test datasets\n",
    "# We don't pass the rare_words parameter, so rare word removal is not performed\n",
    "#data_train['comment_text_clean_2'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_stemming=False))\n",
    "#data_test['comment_text_clean_2'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_stemming=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5336927-d92a-4fd1-acc2-157a5c6c8f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lemmatization (default behavior, without stemming)\n",
    "data_train['text_lemmatization'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=False))\n",
    "\n",
    "# Stemming (enabling stemming, no lemmatization)\n",
    "data_train['text_stemming'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=True, combine_negations=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeff00b0-ba19-4319-a324-a0c540cba267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping semantic punctuation (keeping ! and ?)\n",
    "data_train['text_punctuation'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=True, use_stemming=False, combine_negations=False))\n",
    "\n",
    "# Removing all punctuation\n",
    "data_train['text_no_punctuation'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a5fd7-c294-4b22-b53b-5aa973df0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep negations\n",
    "data_train['text_negations'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=True))\n",
    "# Removing negations\n",
    "data_train['text_no_negations'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc11b6-d602-4f82-9c04-b482b7eaebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only lowercase\n",
    "data_train['data_text_1'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=False, keep_semantic_punctuation=True, use_stemming=False))\n",
    "\n",
    "# Lowercase and stopwords removal\n",
    "data_train['data_text_2'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=True, use_stemming=False))\n",
    "\n",
    "# Add punctuation handling\n",
    "data_train['data_text_3'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False))\n",
    "\n",
    "# Incorporate lemmatization\n",
    "data_train['data_text_4'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=True))\n",
    "\n",
    "# Incorporate stemming\n",
    "data_train['data_text_5'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=True, combine_negations=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc1ab2fc-ecc6-4865-a9d6-8b4962773c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0000997932d777bf    Explanation\\r\\nWhy the edits made under my use...\n",
       "000103f0d9cfb60f    D'aww! He matches this background colour I'm s...\n",
       "000113f07ec002fd    Hey man, I'm really not trying to edit war. It...\n",
       "0001b41b1c6bb37e    \"\\r\\nMore\\r\\nI can't make any real suggestions...\n",
       "0001d958c54c6e35    You, sir, are my hero. Any chance you remember...\n",
       "                                          ...                        \n",
       "ffe987279560d7ff    \":::::And for the second time of asking, when ...\n",
       "ffea4adeee384e90    You should be ashamed of yourself \\r\\n\\r\\nThat...\n",
       "ffee36eab5c267c9    Spitzer \\r\\n\\r\\nUmm, theres no actual article ...\n",
       "fff125370e4aaaf3    And it looks like it was actually you who put ...\n",
       "fff46fc426af1f9a    \"\\r\\nAnd ... I really don't think you understa...\n",
       "Name: comment_text, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2580c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test set\n",
    "X_train = data_train[\"comment_text_clean\"]\n",
    "y_train = data_train[\"is_hate\"]\n",
    "\n",
    "X_test = data_test[\"comment_text_clean\"]\n",
    "y_test = data_test[\"is_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac886fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n"
     ]
    }
   ],
   "source": [
    "maxLen = 0\n",
    "\n",
    "for comment in data_train['comment_text_clean']:\n",
    "    length = len(comment.split())\n",
    "    if(length > maxLen):\n",
    "        maxLen = length\n",
    "\n",
    "print(maxLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f8f7c",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2c013",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Notes**: Tokenizing with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dafd0489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d456ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Convert TF-IDF matrices to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b6a87",
   "metadata": {},
   "source": [
    "## Model Implementation & Test with Testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baace6c",
   "metadata": {},
   "source": [
    "**Note**: Does a CNN makes sense for sentiment analysis? or a simpler model?\n",
    "\n",
    "**Answers and additional Notes**:\n",
    "Make a CNN with PyTorch using skorch as wrapper to make it possible to use sklearn.pipeline with the model\n",
    "This way gridsearch for hyper parameters is possible and tfidfVectorizer can be used for tf-idf\n",
    "CNN: vector size 300, conv. layer of some size, flatten, relu, end with softmax or something\n",
    "Example: https://www.kaggle.com/code/raviusz/jigsaw-toxic-comment\n",
    "example look very good to get basics and then change some of architecture\n",
    "hyperparameter tuning for each model? only if time permits, alt. tune on best model and use for rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ff84f",
   "metadata": {},
   "source": [
    "**Note**: We will use the given test set to compare the different approaches. Make a dataframe with all the results\n",
    "in accuracy, f1, recall, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "970d9db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 0.4149956104898404\n",
      "Epoch [2/7], Loss: 0.41494052894621014\n",
      "Epoch [3/7], Loss: 0.4149405294693397\n",
      "Epoch [4/7], Loss: 0.4149405295686988\n",
      "Epoch [5/7], Loss: 0.41494053030380695\n",
      "Epoch [6/7], Loss: 0.4149405292271051\n",
      "Epoch [7/7], Loss: 0.4149405290705958\n",
      "Test Accuracy: 0.90241958173122\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CNN: The basic model\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=5)\n",
    "\n",
    "        self.conv_output_size = self._get_conv_output_size(input_size)\n",
    "        self.fc1 = nn.Linear(self.conv_output_size, 128)  # Adjust input size based on output of convolutions\n",
    "        self.fc2 = nn.Linear(128, 2)  # Assuming binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def _get_conv_output_size(self, input_size):\n",
    "        x = torch.randn(1, 1, input_size)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool1d(x, 2)\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "\n",
    "batch_size = 25\n",
    "# Step 4: Train the model\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)  # Assuming X_train_tensor and y_train_tensor are tensors\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Move model to GPU\n",
    "model = CNN(1500).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 7\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = total_loss / len(train_dataset)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss}')\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "# Assuming X_test_tensor and y_test_tensor are tensors\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
