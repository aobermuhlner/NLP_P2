{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d484bd",
   "metadata": {},
   "source": [
    "# Project 2 NLP: Hatespeech Classifier\n",
    "\n",
    "## Authors:\n",
    "\n",
    "Adrian Obermühlner & Freja Rasmussen\n",
    "\n",
    "## Resarch Question:\n",
    "\n",
    "How do different preprocessing methods (nothing, stop word removal, lemming, stemming,…) affect the result of a hate speech classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712926f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71084f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizing\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7be0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b6cdf",
   "metadata": {},
   "source": [
    "## Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198a399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BINARY_LABEL = \"is_hate\"\n",
    "CATEGORIES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "np.random.seed(RANDOM_SEED)  # set random seed for reproducibility\n",
    "# Make the labels into hate and no hate as 1 and 0\n",
    "\n",
    "def binarize_labels(df):\n",
    "    return (df[CATEGORIES].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "data_train = pd.read_csv(\"./data/train/train.csv\", index_col=0)\n",
    "data_train[BINARY_LABEL] = binarize_labels(data_train)\n",
    "\n",
    "data_test = pd.read_csv(\"./data/test/test.csv\", index_col=0).join(\n",
    "    pd.read_csv(\"./data/test_labels/test_labels.csv\", index_col=0)\n",
    ")\n",
    "data_test.drop(data_test[data_test[\"toxic\"] == -1].index, inplace=True)\n",
    "data_test[BINARY_LABEL] = binarize_labels(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb59a0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0000997932d777bf    Explanation\\nWhy the edits made under my usern...\n",
       "000103f0d9cfb60f    D'aww! He matches this background colour I'm s...\n",
       "000113f07ec002fd    Hey man, I'm really not trying to edit war. It...\n",
       "0001b41b1c6bb37e    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "0001d958c54c6e35    You, sir, are my hero. Any chance you remember...\n",
       "00025465d4725e87    \"\\n\\nCongratulations from me as well, use the ...\n",
       "0002bcb3da6cb337         COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
       "00031b1e95af7921    Your vandalism to the Matt Shirvington article...\n",
       "00037261f536c51d    Sorry if the word 'nonsense' was offensive to ...\n",
       "00040093b2687caa    alignment on this subject and which are contra...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['comment_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad646cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>is_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001ea8717f6de06</th>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000247e83dcc1211</th>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002f87b16116a7f</th>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003e1cccfd5a40a</th>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00059ace3e3e9a53</th>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000663aff0fffc80</th>\n",
       "      <td>this other one from 1897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000689dd34e20979</th>\n",
       "      <td>== Reason for banning throwing == \\n\\n This ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000844b52dee5f3f</th>\n",
       "      <td>|blocked]] from editing Wikipedia.   |</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00091c35fa9d0465</th>\n",
       "      <td>== Arabs are committing genocide in Iraq, but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000968ce11f5ee34</th>\n",
       "      <td>Please stop. If you continue to vandalize Wiki...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0001ea8717f6de06  Thank you for understanding. I think very high...      0   \n",
       "000247e83dcc1211                   :Dear god this site is horrible.      0   \n",
       "0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...      0   \n",
       "0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...      0   \n",
       "00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...      0   \n",
       "000663aff0fffc80                           this other one from 1897      0   \n",
       "000689dd34e20979  == Reason for banning throwing == \\n\\n This ar...      0   \n",
       "000844b52dee5f3f             |blocked]] from editing Wikipedia.   |      0   \n",
       "00091c35fa9d0465  == Arabs are committing genocide in Iraq, but ...      1   \n",
       "000968ce11f5ee34  Please stop. If you continue to vandalize Wiki...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "id                                                                       \n",
       "0001ea8717f6de06             0        0       0       0              0   \n",
       "000247e83dcc1211             0        0       0       0              0   \n",
       "0002f87b16116a7f             0        0       0       0              0   \n",
       "0003e1cccfd5a40a             0        0       0       0              0   \n",
       "00059ace3e3e9a53             0        0       0       0              0   \n",
       "000663aff0fffc80             0        0       0       0              0   \n",
       "000689dd34e20979             0        0       0       0              0   \n",
       "000844b52dee5f3f             0        0       0       0              0   \n",
       "00091c35fa9d0465             0        0       0       0              0   \n",
       "000968ce11f5ee34             0        0       0       0              0   \n",
       "\n",
       "                  is_hate  \n",
       "id                         \n",
       "0001ea8717f6de06        0  \n",
       "000247e83dcc1211        0  \n",
       "0002f87b16116a7f        0  \n",
       "0003e1cccfd5a40a        0  \n",
       "00059ace3e3e9a53        0  \n",
       "000663aff0fffc80        0  \n",
       "000689dd34e20979        0  \n",
       "000844b52dee5f3f        0  \n",
       "00091c35fa9d0465        1  \n",
       "000968ce11f5ee34        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef08e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of no/is hate for train set:  0    0.898321\n",
      "1    0.101679\n",
      "Name: is_hate, dtype: float64\n",
      "Ratio of no/is hate for test set:  0    0.90242\n",
      "1    0.09758\n",
      "Name: is_hate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# get the distribution of the labels to see if roughly similar for both\n",
    "\n",
    "is_hate_count_train = data_train['is_hate'].value_counts()\n",
    "ratio_train = is_hate_count_train/ len(data_train)\n",
    "\n",
    "is_hate_count_test = data_test['is_hate'].value_counts()\n",
    "ratio_test = is_hate_count_test/ len(data_test)\n",
    "\n",
    "print('Ratio of no/is hate for train set: ', ratio_train)\n",
    "print('Ratio of no/is hate for test set: ', ratio_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41c0af",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96650bb9",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ef87f",
   "metadata": {},
   "source": [
    "**Note**: We would need to make a loop for the different combinations of \n",
    "preprocessing (none, only stemming, only lemming, only stop word removal and every combination of this)\n",
    "Either as coloumns that can be used to iterate over for the model training and validation, or make the preprocessing\n",
    "and then go further and repeat from beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "25fb2d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\flras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\flras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\flras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\flras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def handle_negations(tokens):\n",
    "    negation_words = {'not', \"n't\", 'no', 'never', 'none'}\n",
    "    processed_tokens = []\n",
    "    skip_next = False\n",
    "\n",
    "    for i, word in enumerate(tokens):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        if word in negation_words and i + 1 < len(tokens):\n",
    "            processed_tokens.append(word + '_' + tokens[i + 1])\n",
    "            skip_next = True\n",
    "        else:\n",
    "            processed_tokens.append(word)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Assuming stop_words is defined somewhere\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts treebank tags to wordnet tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def preprocess_text(text, use_lower=True, remove_stopwords=False, use_stemming=False, combine_negations=False, keep_semantic_punctuation=True, rare_words=None):\n",
    "    if use_lower:\n",
    "        text = text.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    if combine_negations:\n",
    "        tokens = handle_negations(tokens)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    if not keep_semantic_punctuation:\n",
    "        tokens = [re.sub(r'[^\\w\\s]', '', word) for word in tokens]\n",
    "    \n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    else:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_tokens]\n",
    "    \n",
    "    filtered_tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to the training and test datasets\n",
    "# We don't pass the rare_words parameter, so rare word removal is not performed\n",
    "#data_train['comment_text_clean_2'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_stemming=False))\n",
    "#data_test['comment_text_clean_2'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_stemming=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25c3c9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "0000997932d777bf    explanation edits make username hardcore metal...\n",
      "000103f0d9cfb60f    daww match background colour m seemingly stuck...\n",
      "000113f07ec002fd    hey man m really try edit war s guy constantly...\n",
      "0001b41b1c6bb37e    ca nt make real suggestion improvement wonder ...\n",
      "0001d958c54c6e35                      sir hero chance remember page s\n",
      "                                          ...                        \n",
      "1a790ff1007a10e3    number may either list separately begin stuck ...\n",
      "1a7a4868968e2b9e                                 two love disagree nt\n",
      "1a7c3bec9a71415d    change lance thomas lance thomas link american...\n",
      "1a7c9c14b0cf0fe0    state court put article deal state law state c...\n",
      "1a7d550fec6e9777                               buddy thing nottingham\n",
      "Name: comment_text, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization (default behavior, without stemming)\n",
    "print(data_train['comment_text'][:10000].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd6594bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e79ff8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization (default behavior, without stemming)\n",
    "data_train['text_lemmatization'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeff00b0-ba19-4319-a324-a0c540cba267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping semantic punctuation (keeping ! and ?)\n",
    "data_train['text_punctuation'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=True, use_stemming=False, combine_negations=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b18892b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a9b2d6e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16300/4212352201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Removing all punctuation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_no_punctuation'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_lower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_semantic_punctuation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_stemming\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombine_negations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_all_coloumns.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3464\u001b[0m         )\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3466\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m             )\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         libwriters.write_csv_rows(\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\pandas\\_libs\\writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Removing all punctuation\n",
    "data_train['text_no_punctuation'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=False))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd7a5fd7-c294-4b22-b53b-5aa973df0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep negations\n",
    "data_train['text_negations'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=True))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8dc11b6-d602-4f82-9c04-b482b7eaebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only lowercase\n",
    "data_train['data_text_1'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=False, keep_semantic_punctuation=True, use_stemming=False))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dd65387",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lowercase and stopwords removal\n",
    "data_train['data_text_2'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=True, use_stemming=False))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70e8467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add punctuation handling\n",
    "data_train['data_text_3'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e308133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Incorporate lemmatization\n",
    "data_train['data_text_4'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=True))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049def12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Incorporate stemming\n",
    "data_train['data_text_5'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=True, combine_negations=True))\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dcc9971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "0000997932d777bf    explan edit made usernam hardcor metallica fan...\n",
      "000103f0d9cfb60f    daww match background colour m seemingli stuck...\n",
      "000113f07ec002fd    hey man m realli edit war s guy constantli rem...\n",
      "0001b41b1c6bb37e    ca real suggest improv wonder section statist ...\n",
      "0001d958c54c6e35                         sir hero chanc rememb page s\n",
      "                                          ...                        \n",
      "172e07f9ab48bb39    refer book kemantney languag howev figur confi...\n",
      "172e16e4f8de66fe    line text three section list offic proport rea...\n",
      "1730ef89d087b7c8                                  say countri countri\n",
      "1731c1aae5f1cb32    renata go russian websit find work publish eve...\n",
      "1731fcac2661469b    stop remov matter talk perman makeup edit war ...\n",
      "Name: comment_text, Length: 100000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "testing = data_train['comment_text'][:100000].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=True, combine_negations=True))\n",
    "print(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f762d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>is_hate</th>\n",
       "      <th>text_lemmatization</th>\n",
       "      <th>text_punctuation</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "      <th>text_negations</th>\n",
       "      <th>data_text_1</th>\n",
       "      <th>data_text_2</th>\n",
       "      <th>data_text_3</th>\n",
       "      <th>data_text_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000997932d777bf</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation edits make username hardcore metal...</td>\n",
       "      <td>explanation edits make username hardcore metal...</td>\n",
       "      <td>explanation edits make username hardcore metal...</td>\n",
       "      <td>explanation edits make username hardcore metal...</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "      <td>explanation edits make username hardcore metal...</td>\n",
       "      <td>explanation edits make username hardcore metal...</td>\n",
       "      <td>explanation edits make username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000103f0d9cfb60f</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daww match background colour m seemingly stuck...</td>\n",
       "      <td>match background colour seemingly stick thanks...</td>\n",
       "      <td>daww match background colour m seemingly stuck...</td>\n",
       "      <td>daww match background colour m seemingly stuck...</td>\n",
       "      <td>he match this background colour i seemingly st...</td>\n",
       "      <td>match background colour seemingly stick thanks...</td>\n",
       "      <td>daww match background colour m seemingly stuck...</td>\n",
       "      <td>daww match background colour m seemingly stuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000113f07ec002fd</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man m really try edit war s guy constantly...</td>\n",
       "      <td>hey man really try edit war guy constantly rem...</td>\n",
       "      <td>hey man m really try edit war s guy constantly...</td>\n",
       "      <td>hey man m really edit war s guy constantly rem...</td>\n",
       "      <td>hey man i really not try to edit war it just t...</td>\n",
       "      <td>hey man really try edit war guy constantly rem...</td>\n",
       "      <td>hey man m really try edit war s guy constantly...</td>\n",
       "      <td>hey man m really edit war s guy constantly rem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "id                                                                       \n",
       "0000997932d777bf             0        0       0       0              0   \n",
       "000103f0d9cfb60f             0        0       0       0              0   \n",
       "000113f07ec002fd             0        0       0       0              0   \n",
       "\n",
       "                  is_hate                                 text_lemmatization  \\\n",
       "id                                                                             \n",
       "0000997932d777bf        0  explanation edits make username hardcore metal...   \n",
       "000103f0d9cfb60f        0  daww match background colour m seemingly stuck...   \n",
       "000113f07ec002fd        0  hey man m really try edit war s guy constantly...   \n",
       "\n",
       "                                                   text_punctuation  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation edits make username hardcore metal...   \n",
       "000103f0d9cfb60f  match background colour seemingly stick thanks...   \n",
       "000113f07ec002fd  hey man really try edit war guy constantly rem...   \n",
       "\n",
       "                                                text_no_punctuation  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation edits make username hardcore metal...   \n",
       "000103f0d9cfb60f  daww match background colour m seemingly stuck...   \n",
       "000113f07ec002fd  hey man m really try edit war s guy constantly...   \n",
       "\n",
       "                                                     text_negations  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation edits make username hardcore metal...   \n",
       "000103f0d9cfb60f  daww match background colour m seemingly stuck...   \n",
       "000113f07ec002fd  hey man m really edit war s guy constantly rem...   \n",
       "\n",
       "                                                        data_text_1  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation why the edits make under my userna...   \n",
       "000103f0d9cfb60f  he match this background colour i seemingly st...   \n",
       "000113f07ec002fd  hey man i really not try to edit war it just t...   \n",
       "\n",
       "                                                        data_text_2  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation edits make username hardcore metal...   \n",
       "000103f0d9cfb60f  match background colour seemingly stick thanks...   \n",
       "000113f07ec002fd  hey man really try edit war guy constantly rem...   \n",
       "\n",
       "                                                        data_text_3  \\\n",
       "id                                                                    \n",
       "0000997932d777bf  explanation edits make username hardcore metal...   \n",
       "000103f0d9cfb60f  daww match background colour m seemingly stuck...   \n",
       "000113f07ec002fd  hey man m really try edit war s guy constantly...   \n",
       "\n",
       "                                                        data_text_4  \n",
       "id                                                                   \n",
       "0000997932d777bf  explanation edits make username hardcore metal...  \n",
       "000103f0d9cfb60f  daww match background colour m seemingly stuck...  \n",
       "000113f07ec002fd  hey man m really edit war s guy constantly rem...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"./train_all_coloumns.csv\", index_col=0)\n",
    "\n",
    "data_test = pd.read_csv(\"./test_all_coloumns.csv\", index_col=0)\n",
    "data_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da93c2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cocksucker piss around work (159571, 16) (63978, 16)\n"
     ]
    }
   ],
   "source": [
    "print(data_train['data_text_4'][6], data_train.shape, data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Incorporate stemming\n",
    "data_train['data_text_5'] = data_train['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=True, combine_negations=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7022466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization (default behavior, without stemming)\n",
    "data_test['text_lemmatization'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=False))\n",
    "data_test.to_csv('test_all_coloumns.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3de0075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization (default behavior, without stemming)\n",
    "data_test['text_punctuation'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=False))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2ad32233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Removing all punctuation\n",
    "data_test['text_punctuation'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=True, use_stemming=False, combine_negations=False))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef429a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stemming (enabling stemming, no lemmatization)\n",
    "data_test['text_stemming'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=True, combine_negations=False))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0ad3aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all punctuation\n",
    "data_test['text_no_punctuation'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=False))\n",
    "data_test.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c859d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep negations\n",
    "data_test['text_negations'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=True))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac75348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Removing negations\n",
    "data_test['text_no_negations'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=False))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c33db94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Only lowercase\n",
    "data_test['data_text_1'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=False, keep_semantic_punctuation=True, use_stemming=False))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0544f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lowercase and stopwords removal\n",
    "data_test['data_text_2'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=True, use_stemming=False))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32d66174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add punctuation handling\n",
    "data_test['data_text_3'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e58fc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Incorporate lemmatization\n",
    "data_test['data_text_4'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=False, combine_negations=True))\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26cf0bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Incorporate stemming\n",
    "data_test['data_text_5'] = data_test['comment_text'].apply(lambda x: preprocess_text(x, use_lower=True, remove_stopwords=True, keep_semantic_punctuation=False, use_stemming=True, combine_negations=True))\n",
    "\n",
    "data_test.to_csv('test_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54090b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_csv('test_all_coloumns.csv')\n",
    "data_train.to_csv('train_all_coloumns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cc1ab2fc-ecc6-4865-a9d6-8b4962773c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0001ea8717f6de06    thank understanding think highly would revert ...\n",
       "000247e83dcc1211                               dear god site horrible\n",
       "0002f87b16116a7f    somebody invariably try add religion really me...\n",
       "0003e1cccfd5a40a    say right type type institution need case thre...\n",
       "00059ace3e3e9a53    add new product list make sure relevant add ne...\n",
       "                                          ...                        \n",
       "fff8f64043129fa2    jerome see never get around surprise looked ex...\n",
       "fff9d70fe0722906                   http heh famous kida envy congrats\n",
       "fffa8a11c4378854                              want speak gay romanian\n",
       "fffac2a094c8e0e2    mel gibson nazi bitch make shitty movie much b...\n",
       "fffb5451268fb5ba    unicorn lair discovery supposedly lair discove...\n",
       "Name: text_punctuation, Length: 63978, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_test['text_punctuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35bf8195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_text             0\n",
      "toxic                    0\n",
      "severe_toxic             0\n",
      "obscene                  0\n",
      "threat                   0\n",
      "insult                   0\n",
      "identity_hate            0\n",
      "is_hate                  0\n",
      "text_lemmatization      63\n",
      "text_punctuation       155\n",
      "text_no_punctuation     63\n",
      "text_negations         115\n",
      "data_text_1             65\n",
      "data_text_2            155\n",
      "data_text_3             63\n",
      "data_text_4            115\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2580c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test set\n",
    "# Define test and train set\n",
    "def datasetDefinition(columnName):\n",
    "    X_train = data_train[columnName]\n",
    "    y_train = data_train[\"is_hate\"]\n",
    "\n",
    "    X_test = data_test[columnName]\n",
    "    y_test = data_test[\"is_hate\"]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f8f7c",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2c013",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Notes**: Tokenizing with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dafd0489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d456ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeToTensors(X_train, y_train, X_test, y_test):\n",
    "    # Make the test and train sets to tensors and apply TF-IDF\n",
    "    X_train.fillna('', inplace=True)\n",
    "    X_test.fillna('', inplace=True)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Transform testing data\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # Convert TF-IDF matrices to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b6a87",
   "metadata": {},
   "source": [
    "## Model Implementation & Test with Testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baace6c",
   "metadata": {},
   "source": [
    "**Note**: Does a CNN makes sense for sentiment analysis? or a simpler model?\n",
    "\n",
    "**Answers and additional Notes**:\n",
    "Make a CNN with PyTorch using skorch as wrapper to make it possible to use sklearn.pipeline with the model\n",
    "This way gridsearch for hyper parameters is possible and tfidfVectorizer can be used for tf-idf\n",
    "CNN: vector size 300, conv. layer of some size, flatten, relu, end with softmax or something\n",
    "Example: https://www.kaggle.com/code/raviusz/jigsaw-toxic-comment\n",
    "example look very good to get basics and then change some of architecture\n",
    "hyperparameter tuning for each model? only if time permits, alt. tune on best model and use for rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ff84f",
   "metadata": {},
   "source": [
    "**Note**: We will use the given test set to compare the different approaches. Make a dataframe with all the results\n",
    "in accuracy, f1, recall, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "970d9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CNN: The basic model\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=5)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv_output_size = self._get_conv_output_size(2000)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.conv_output_size, 64)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "    def _get_conv_output_size(self, input_size):\n",
    "        x = torch.randn(1, 1, input_size)  # Add channel dimension\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "\n",
    "batchSize = 25\n",
    "\n",
    "def trainCNN(X_train, y_train, X_test, y_test,  batch_size=batchSize, epochs=7, learning_rate=0.001):\n",
    "\n",
    "    # Step 4: Train the model\n",
    "    train_dataset = TensorDataset(X_train, y_train)  # Assuming X_train_tensor and y_train_tensor are tensors\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    # Move model to GPU\n",
    "    model = CNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(f'learning rate: {learning_rate}, total number of epochs: {epochs}')\n",
    "    for epoch in range(int(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = total_loss / len(train_dataset)\n",
    "        epoch_accuracy = correct / total\n",
    "        train_loss.append(epoch_loss)\n",
    "        train_accuracy.append(epoch_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss}, Accuracy: {epoch_accuracy}')\n",
    "\n",
    "    # Step 5: Evaluate the model\n",
    "    # Assuming X_test_tensor and y_test_tensor are tensors\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_predicted = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += torch.sum(predicted == labels).item()\n",
    "            total_predicted += len(predicted)\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = total_correct / total_predicted\n",
    "    \n",
    "\n",
    "    return accuracy, model, train_loss, train_accuracy\n",
    "    print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fea5efae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.0005, total number of epochs: 15\n",
      "Epoch [1/15], Loss: 0.4151755427423488, Accuracy: 0.8982145878637096\n",
      "Epoch [2/15], Loss: 0.4149526729235221, Accuracy: 0.8983211235124177\n",
      "Epoch [3/15], Loss: 0.4149413854670959, Accuracy: 0.8983211235124177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16300/1009125598.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Get the test accuracy using the best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mall_train_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16300/2834154267.py\u001b[0m in \u001b[0;36mtrainCNN\u001b[1;34m(X_train, y_train, X_test, y_test, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    142\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for all the columns load them into X_train\n",
    "# make the tensors, train the cnn and thenn take the outcome and add it to a df\n",
    "columnName = ['text_lemmatization', 'text_punctuation', 'text_no_punctuation', 'text_negations', 'text_no_negations', 'data_text_1', 'data_text_2',\n",
    "            'data_text_2', 'data_text_3', 'data_text_4']\n",
    "results = []\n",
    "all_train_loss = []\n",
    "all_train_accuracy = []\n",
    "for name in columnName:\n",
    "    X_train, y_train, X_test, y_test = datasetDefinition(name)\n",
    "    X_train_T, y_train_T, X_test_T, y_test_T = makeToTensors(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Get the test accuracy using the best model\n",
    "    accuracy, model, train_loss, train_accuracy = trainCNN(X_train_T, y_train_T, X_test_T, y_test_T, epochs=15, learning_rate=0.0005)\n",
    "    \n",
    "    all_train_loss.append(train_loss)\n",
    "    all_train_accuracy.append(train_accuracy)\n",
    "\n",
    "    plt.plot(train_loss, label=f'{name} Loss')\n",
    "    plt.plot(train_accuracy, label=f'{name} Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Loss and Accuracy vs. Epochs for {name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    results.append({'Column': name, 'Test_Accuracy': accuracy})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('hyperparameter_tuning_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905d948",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16300/477429924.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumnName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_train_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'{name} Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epochs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "columnName = ['text_lemmatization', 'text_punctuation', 'text_no_punctuation', 'text_negations', 'text_no_negations', 'data_text_1', 'data_text_2',\n",
    "            'data_text_2', 'data_text_3', 'data_text_4']\n",
    "\n",
    "for i, name in enumerate(columnName):\n",
    "    plt.plot(all_train_loss[i], label=f'{name} Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs. Epochs for Different Preprocessing Methods')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i, name in enumerate(columnName):\n",
    "    plt.plot(all_train_accuracy[i], label=f'{name} Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy vs. Epochs for Different Preprocessing Methods')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ceef2092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_lemmatization               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96     57735\n",
      "           1       0.64      0.74      0.68      6243\n",
      "\n",
      "    accuracy                           0.93     63978\n",
      "   macro avg       0.80      0.85      0.82     63978\n",
      "weighted avg       0.94      0.93      0.94     63978\n",
      "\n",
      "text_punctuation               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96     57735\n",
      "           1       0.64      0.74      0.68      6243\n",
      "\n",
      "    accuracy                           0.93     63978\n",
      "   macro avg       0.80      0.85      0.82     63978\n",
      "weighted avg       0.94      0.93      0.94     63978\n",
      "\n",
      "text_no_punctuation               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96     57735\n",
      "           1       0.64      0.74      0.68      6243\n",
      "\n",
      "    accuracy                           0.93     63978\n",
      "   macro avg       0.80      0.85      0.82     63978\n",
      "weighted avg       0.94      0.93      0.94     63978\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16300/1074671081.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumnName\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasetDefinition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mlinear_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1844\u001b[0m         \"\"\"\n\u001b[0;32m   1845\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1846\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1847\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1203\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flras\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    218\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "columnName = ['text_lemmatization', 'text_punctuation', 'text_no_punctuation', 'text_negations', 'text_no_negations', 'data_text_1', 'data_text_2',\n",
    "            'data_text_2', 'data_text_3', 'data_text_4']\n",
    "\n",
    "classification_reports = []\n",
    "linear_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    LogisticRegression(solver='sag', max_iter=1000)\n",
    ")\n",
    "for name in columnName:\n",
    "    X_train, y_train, X_test, y_test = datasetDefinition(name)\n",
    "    linear_pipeline.fit(X_train, y_train)\n",
    "    y_pred = linear_pipeline.predict(X_test)\n",
    "    print(name, classification_report(y_test, y_pred))\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    classification_reports.append(report_df)\n",
    "    \n",
    "classification_reports_df = pd.concat(classification_reports, keys=columnName)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "classification_reports_df.to_csv('classification_reports.csv')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for name in columnName:\n",
    "    plt.plot(classification_reports_df.loc[name]['accuracy'], label=name, marker='o')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot recall for each dataset\n",
    "plt.figure(figsize=(12, 8))\n",
    "for name in columnName:\n",
    "    plt.plot(classification_reports_df.loc[name]['recall'], label=name, marker='o')\n",
    "plt.title('Recall')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot F1-score for each dataset\n",
    "plt.figure(figsize=(12, 8))\n",
    "for name in columnName:\n",
    "    plt.plot(classification_reports_df.loc[name]['f1-score'], label=name, marker='o')\n",
    "plt.title('F1-score')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('F1-score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8d03375b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0000997932d777bf    explanation edits make username hardcore metal...\n",
       "000103f0d9cfb60f    daww match background colour m seemingly stuck...\n",
       "000113f07ec002fd    hey man m really edit war s guy constantly rem...\n",
       "0001b41b1c6bb37e    ca real suggestion improvement wonder section ...\n",
       "0001d958c54c6e35                      sir hero chance remember page s\n",
       "                                          ...                        \n",
       "ffe987279560d7ff    second time ask view completely contradict cov...\n",
       "ffea4adeee384e90                 ashamed horrible thing put talk page\n",
       "ffee36eab5c267c9    spitzer umm theres article prostitution ring c...\n",
       "fff125370e4aaaf3    look like actually put speedy first version de...\n",
       "fff46fc426af1f9a    really understand come idea bad right away kin...\n",
       "Name: text_negations, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['text_negations']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
