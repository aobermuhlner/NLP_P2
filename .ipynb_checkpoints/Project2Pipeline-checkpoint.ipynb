{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d484bd",
   "metadata": {},
   "source": [
    "# Project 2 NLP: Hatespeech Classifier\n",
    "\n",
    "## Authors:\n",
    "\n",
    "Adrian Obermühlner & Freja Rasmussen\n",
    "\n",
    "## Resarch Question:\n",
    "\n",
    "How do different preprocessing methods (nothing, stop word removal, lemming, stemming,…) affect the result of a hate speech classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712926f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71084f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Preprocessing imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizing\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.keyedvectors import Vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7be0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b6cdf",
   "metadata": {},
   "source": [
    "## Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198a399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BINARY_LABEL = \"is_hate\"\n",
    "CATEGORIES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "np.random.seed(RANDOM_SEED)  # set random seed for reproducibility\n",
    "# Make the labels into hate and no hate as 1 and 0\n",
    "\n",
    "def binarize_labels(df):\n",
    "    return (df[CATEGORIES].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "data_train = pd.read_csv(\"./data/train/train.csv\", index_col=0)\n",
    "data_train[BINARY_LABEL] = binarize_labels(data_train)\n",
    "\n",
    "data_test = pd.read_csv(\"./data/test/test.csv\", index_col=0).join(\n",
    "    pd.read_csv(\"./data/test_labels/test_labels.csv\", index_col=0)\n",
    ")\n",
    "data_test.drop(data_test[data_test[\"toxic\"] == -1].index, inplace=True)\n",
    "data_test[BINARY_LABEL] = binarize_labels(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb59a0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0000997932d777bf    Explanation\\nWhy the edits made under my usern...\n",
       "000103f0d9cfb60f    D'aww! He matches this background colour I'm s...\n",
       "000113f07ec002fd    Hey man, I'm really not trying to edit war. It...\n",
       "0001b41b1c6bb37e    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "0001d958c54c6e35    You, sir, are my hero. Any chance you remember...\n",
       "00025465d4725e87    \"\\n\\nCongratulations from me as well, use the ...\n",
       "0002bcb3da6cb337         COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
       "00031b1e95af7921    Your vandalism to the Matt Shirvington article...\n",
       "00037261f536c51d    Sorry if the word 'nonsense' was offensive to ...\n",
       "00040093b2687caa    alignment on this subject and which are contra...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['comment_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad646cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>is_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001ea8717f6de06</th>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000247e83dcc1211</th>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002f87b16116a7f</th>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003e1cccfd5a40a</th>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00059ace3e3e9a53</th>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000663aff0fffc80</th>\n",
       "      <td>this other one from 1897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000689dd34e20979</th>\n",
       "      <td>== Reason for banning throwing == \\n\\n This ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000844b52dee5f3f</th>\n",
       "      <td>|blocked]] from editing Wikipedia.   |</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00091c35fa9d0465</th>\n",
       "      <td>== Arabs are committing genocide in Iraq, but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000968ce11f5ee34</th>\n",
       "      <td>Please stop. If you continue to vandalize Wiki...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0001ea8717f6de06  Thank you for understanding. I think very high...      0   \n",
       "000247e83dcc1211                   :Dear god this site is horrible.      0   \n",
       "0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...      0   \n",
       "0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...      0   \n",
       "00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...      0   \n",
       "000663aff0fffc80                           this other one from 1897      0   \n",
       "000689dd34e20979  == Reason for banning throwing == \\n\\n This ar...      0   \n",
       "000844b52dee5f3f             |blocked]] from editing Wikipedia.   |      0   \n",
       "00091c35fa9d0465  == Arabs are committing genocide in Iraq, but ...      1   \n",
       "000968ce11f5ee34  Please stop. If you continue to vandalize Wiki...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "id                                                                       \n",
       "0001ea8717f6de06             0        0       0       0              0   \n",
       "000247e83dcc1211             0        0       0       0              0   \n",
       "0002f87b16116a7f             0        0       0       0              0   \n",
       "0003e1cccfd5a40a             0        0       0       0              0   \n",
       "00059ace3e3e9a53             0        0       0       0              0   \n",
       "000663aff0fffc80             0        0       0       0              0   \n",
       "000689dd34e20979             0        0       0       0              0   \n",
       "000844b52dee5f3f             0        0       0       0              0   \n",
       "00091c35fa9d0465             0        0       0       0              0   \n",
       "000968ce11f5ee34             0        0       0       0              0   \n",
       "\n",
       "                  is_hate  \n",
       "id                         \n",
       "0001ea8717f6de06        0  \n",
       "000247e83dcc1211        0  \n",
       "0002f87b16116a7f        0  \n",
       "0003e1cccfd5a40a        0  \n",
       "00059ace3e3e9a53        0  \n",
       "000663aff0fffc80        0  \n",
       "000689dd34e20979        0  \n",
       "000844b52dee5f3f        0  \n",
       "00091c35fa9d0465        1  \n",
       "000968ce11f5ee34        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef08e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of no/is hate for train set:  0    0.898321\n",
      "1    0.101679\n",
      "Name: is_hate, dtype: float64\n",
      "Ratio of no/is hate for test set:  0    0.90242\n",
      "1    0.09758\n",
      "Name: is_hate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# get the distribution of the labels to see if roughly similar for both\n",
    "\n",
    "is_hate_count_train = data_train['is_hate'].value_counts()\n",
    "ratio_train = is_hate_count_train/ len(data_train)\n",
    "\n",
    "is_hate_count_test = data_test['is_hate'].value_counts()\n",
    "ratio_test = is_hate_count_test/ len(data_test)\n",
    "\n",
    "print('Ratio of no/is hate for train set: ', ratio_train)\n",
    "print('Ratio of no/is hate for test set: ', ratio_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41c0af",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f2beba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test set\n",
    "X_train = data_train[\"comment_text\"]\n",
    "y_train = data_train[\"is_hate\"]\n",
    "\n",
    "X_test = data_test[\"comment_text\"]\n",
    "y_test = data_test[\"is_hate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96650bb9",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ef87f",
   "metadata": {},
   "source": [
    "**Note**: We would need to make a loop for the different combinations of \n",
    "preprocessing (none, only stemming, only lemming, only stop word removal and every combination of this)\n",
    "Either as coloumns that can be used to iterate over for the model training and validation, or make the preprocessing\n",
    "and then go further and repeat from beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb2d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Lowercase all text    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and apply stemming\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemming of words\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens if word.isalpha()]\n",
    "    \n",
    "    # Join the stemmed words back into a sentence\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "data_train['comment_text_clean'] = data_train['comment_text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81320722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "max_len_train = max(len(text) for text in data_train['comment_text'])\n",
    "print(max_len_train)\n",
    "max_len_test = max(len(text) for text in data_test['comment_text'])\n",
    "print(max_len_test)\n",
    "max_len = max(max_len_train, max_len_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f8f7c",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2c013",
   "metadata": {},
   "source": [
    "**Note**: How is word embedding implemented? Or is it better to use TF-IDF?\n",
    "\n",
    "**Additional Notes and Answers**: Tokenizing with TF-IDF or alternatively\n",
    "Look at example of word embedding with cnn for pytorch or tensorflow\n",
    "fast text für word embedding from facebook, maybe shorten down to only words in data set, don't forget to add padding token\n",
    "Tokenize the comment, hand the vector with padding(for longest comment in batch so all have same size) to CNN\n",
    "in CNN first layer is embedding, this layer genrates matrix with size vocab dimensions x length of longest comment\n",
    "Then use CNN as desired for output with softmax at the end for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dafd0489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "babfb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Extract unique words from your dataset\n",
    "unique_words = set()\n",
    "for sentence in data_train[\"comment_text\"][:10000]:\n",
    "    unique_words.update(sentence.split())\n",
    "\n",
    "# Step 2: Filter FastText model\n",
    "filtered_embeddings = {}\n",
    "for word in unique_words:\n",
    "    if word in fasttext_model:\n",
    "        filtered_embeddings[word] = fasttext_model[word]\n",
    "\n",
    "# Step 3: Create a new FastText model using only filtered embeddings\n",
    "filtered_fasttext_model = KeyedVectors(vector_size=300)\n",
    "word_to_index = {word: i for i, word in enumerate(filtered_embeddings.keys())}\n",
    "index_to_word = list(filtered_embeddings.keys())\n",
    "vectors = np.array(list(filtered_embeddings.values()))\n",
    "\n",
    "# Assign filtered embeddings to the new model\n",
    "filtered_fasttext_model.add_vectors(keys=index_to_word, weights=vectors, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff12ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences:  72%|███████▏  | 7182/10000 [01:33<00:40, 69.43it/s] "
     ]
    }
   ],
   "source": [
    "# Tokenize and convert text data to numerical sequences\n",
    "def wordVectors(sentence):\n",
    "    # Tokenize the sentence and filter out words not in the FastText model\n",
    "    sentence_tokens = [word for word in sentence.split() if word in filtered_fasttext_model]\n",
    "    \n",
    "    # Convert each token to its FastText embedding\n",
    "    sentence_embeddings = [fasttext_model[word] for word in sentence_tokens]\n",
    "    \n",
    "    # Pad the sentence embeddings to the maximum length\n",
    "    padded_embeddings = sentence_embeddings + [np.zeros(300)] * (max_len - len(sentence_tokens))\n",
    "    \n",
    "    # Convert the padded embeddings to a single numpy array\n",
    "    padded_embeddings_np = np.array(padded_embeddings, dtype=np.float32)\n",
    "    \n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    X_tensor = torch.tensor(padded_embeddings_np)\n",
    "    \n",
    "    return X_tensor\n",
    "\n",
    "# Apply wordVectors to each item in X_train with progress bar\n",
    "X_train_tensors = []\n",
    "for sentence in tqdm(X_train[:10000], desc=\"Processing sentences\"):\n",
    "    X_train_tensors.append(wordVectors(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensors = []\n",
    "for sentence in tqdm(X_test[:1000], desc=\"Processing sentences\"):\n",
    "    X_test_tensors.append(wordVectors(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize an empty list to store batches of tensors\n",
    "X_train_batches = []\n",
    "\n",
    "# Process the data in batches\n",
    "for i in range(0, len(X_train_tensors), batch_size):\n",
    "    batch = X_train_tensors[i:i+batch_size]  # Extract a batch of tensors\n",
    "    stacked_batch = torch.stack(batch)  # Stack the batch of tensors\n",
    "    X_train_batches.append(stacked_batch)  # Add the batch to the list\n",
    "\n",
    "# Convert y_train to a tensor\n",
    "y_train_tensor = torch.tensor(y_train[:10000], dtype=torch.float32)\n",
    "\n",
    "# Combine the list of batches into a single tensor\n",
    "X_train_tensor = torch.cat(X_train_batches, dim=0)\n",
    "\n",
    "# Move tensors to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "\n",
    "# Define data loader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_filters, filter_sizes, output_dim, dropout):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(fasttext_model.vectors))\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x.permute(0, 2, 1))  # Permute dimensions for Conv1d\n",
    "        conved = [torch.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [torch.max(conv, dim=2)[0] for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        output = self.fc(cat)\n",
    "        return output\n",
    "\n",
    "# Initialize model and move it to the GPU if available\n",
    "model = CNN(embedding_dim, num_filters, filter_sizes, output_dim, dropout)\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move inputs and targets to GPU\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Evaluation (Assuming you have defined test_loader similarly)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move inputs and targets to GPU\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total_correct += (predicted == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b6a87",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf4676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3baace6c",
   "metadata": {},
   "source": [
    "**Note**: Does a CNN makes sense for sentiment analysis? or a simpler model?\n",
    "\n",
    "**Answers and additional Notes**:\n",
    "Make a CNN with PyTorch using skorch as wrapper to make it possible to use sklearn.pipeline with the model\n",
    "This way gridsearch for hyper parameters is possible and tfidfVectorizer can be used for tf-idf\n",
    "CNN: vector size 300, conv. layer of some size, flatten, relu, end with softmax or something\n",
    "Example: https://www.kaggle.com/code/raviusz/jigsaw-toxic-comment\n",
    "example look very good to get basics and then change some of architecture\n",
    "hyperparameter tuning for each model? only if time permits, alt. tune on best model and use for rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb3f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 60\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(layers.Embedding(max_words, embedding_dim, input_length=max_len)) #vocab size, embed vector size, input size\n",
    "\n",
    "# Convolutional layers with different filter sizes\n",
    "model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "\n",
    "# GlobalMaxPooling\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# Dense Layer\n",
    "model.add(layers.Dense(units=20, activation='relu'))\n",
    "\n",
    "# Output layer for classification\n",
    "model.add(layers.Dense(units=2, activation='sigmoid'))\n",
    "\n",
    "# Define the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3b388",
   "metadata": {},
   "source": [
    "## Test with Testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ff84f",
   "metadata": {},
   "source": [
    "**Note**: We will use the given test set to compare the different approaches. Make a dataframe with all the results\n",
    "in accuracy, f1, recall, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e984d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d938347",
   "metadata": {},
   "source": [
    "**Note**: Setfit as example of what to use if I do not take CNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
