{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "BINARY_LABEL = \"is_hate\"\n",
    "CATEGORIES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "np.random.seed(RANDOM_SEED)  # set random seed for reproducibility\n",
    "\n",
    "def binarize_labels(df):\n",
    "    return (df[CATEGORIES].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "data_train = pd.read_csv(\"../data/hatespeech/train.csv\", index_col=0)\n",
    "data_train[BINARY_LABEL] = binarize_labels(data_train)\n",
    "\n",
    "data_test = pd.read_csv(\"../data/hatespeech/test.csv\", index_col=0).join(\n",
    "    pd.read_csv(\"../data/hatespeech/test_labels.csv\", index_col=0)\n",
    ")\n",
    "data_test.drop(data_test[data_test[\"toxic\"] == -1].index, inplace=True)\n",
    "data_test[BINARY_LABEL] = binarize_labels(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train[\"comment_text\"]\n",
    "y_train = data_train[\"is_hate\"]\n",
    "\n",
    "X_test = data_test[\"comment_text\"]\n",
    "y_test = data_test[\"is_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "svm_pipeline = make_pipeline(TfidfVectorizer(), SGDClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96     57735\n",
      "           1       0.66      0.71      0.68      6243\n",
      "\n",
      "    accuracy                           0.94     63978\n",
      "   macro avg       0.81      0.84      0.82     63978\n",
      "weighted avg       0.94      0.94      0.94     63978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear_pipeline.fit(X_train, y_train)\n",
    "y_pred = linear_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     57735\n",
      "           1       0.79      0.52      0.63      6243\n",
      "\n",
      "    accuracy                           0.94     63978\n",
      "   macro avg       0.87      0.75      0.80     63978\n",
      "weighted avg       0.94      0.94      0.93     63978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_pipeline.fit(X_train, y_train)\n",
    "y_pred = svm_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SetFit Examples\n",
    "\n",
    "Attention: Typically requires a GPU to train -> uses small sample of dataset for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import SetFitModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(\n",
    "    # take 10 samples from each class\n",
    "    data_train.groupby(\"is_hate\").sample(10, random_state=RANDOM_SEED)\n",
    ")\n",
    "dataset_test = Dataset.from_pandas(\n",
    "    # take 100 samples from each class\n",
    "    data_test.groupby(\"is_hate\").sample(100, random_state=RANDOM_SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92d84ab88cf4737908cafd0b0fe3487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    labels=[0, 1],\n",
    ").to(\"cpu\")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    batch_size=1,\n",
    "    num_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    num_iterations=20,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    metric=\"accuracy\",\n",
    "    column_mapping={\"comment_text\": \"text\", \"is_hate\": \"label\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 800\n",
      "  Batch size = 1\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d1aa8bb8804b60b59d8c33a1f93e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42f4cc5e04d406ba1d14e9c7cae03dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.5138, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.0}\n",
      "{'embedding_loss': 0.0554, 'learning_rate': 1.25e-05, 'epoch': 0.06}\n",
      "{'embedding_loss': 0.0001, 'learning_rate': 1.9444444444444445e-05, 'epoch': 0.12}\n",
      "{'embedding_loss': 0.0312, 'learning_rate': 1.8055555555555558e-05, 'epoch': 0.19}\n",
      "{'embedding_loss': 0.0025, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.25}\n",
      "{'embedding_loss': 0.0003, 'learning_rate': 1.5277777777777777e-05, 'epoch': 0.31}\n",
      "{'embedding_loss': 0.0, 'learning_rate': 1.388888888888889e-05, 'epoch': 0.38}\n",
      "{'embedding_loss': 0.0, 'learning_rate': 1.25e-05, 'epoch': 0.44}\n",
      "{'embedding_loss': 0.0008, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.5}\n",
      "{'embedding_loss': 0.0002, 'learning_rate': 9.722222222222223e-06, 'epoch': 0.56}\n",
      "{'embedding_loss': 0.0, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.62}\n",
      "{'embedding_loss': 0.0002, 'learning_rate': 6.944444444444445e-06, 'epoch': 0.69}\n",
      "{'embedding_loss': 0.0001, 'learning_rate': 5.555555555555557e-06, 'epoch': 0.75}\n",
      "{'embedding_loss': 0.0, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.81}\n",
      "{'embedding_loss': 0.0, 'learning_rate': 2.7777777777777783e-06, 'epoch': 0.88}\n",
      "{'embedding_loss': 0.0, 'learning_rate': 1.3888888888888892e-06, 'epoch': 0.94}\n",
      "{'embedding_loss': 0.0001, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c759c3a72844b59b0e4862f9000bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading best SentenceTransformer model from step 800.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_embedding_loss': 0.2344, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 308.3876, 'train_samples_per_second': 2.594, 'train_steps_per_second': 2.594, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.775}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74       100\n",
      "           1       0.72      0.90      0.80       100\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.79      0.78      0.77       200\n",
      "weighted avg       0.79      0.78      0.77       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = dataset_test[\"is_hate\"]\n",
    "y_pred_setfit = model.predict(dataset_test[\"comment_text\"])\n",
    "print(classification_report(y_true, y_pred_setfit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Examplwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96     57735\n",
      "           1       0.63      0.70      0.66      6243\n",
      "\n",
      "    accuracy                           0.93     63978\n",
      "   macro avg       0.80      0.83      0.81     63978\n",
      "weighted avg       0.93      0.93      0.93     63978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgboost_pipeline = make_pipeline(TfidfVectorizer(), XGBClassifier())\n",
    "xgboost_pipeline.fit(X_train, y_train)\n",
    "y_pred = xgboost_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
